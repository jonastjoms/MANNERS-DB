{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,argparse,time\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "tstart=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Arguments =\n",
      "\tseed: 0\n",
      "\tdevice: cpu\n",
      "\texperiment: 1_task_groups\n",
      "\tapproach: PUGCL\n",
      "\tdata_path: data/data.csv\n",
      "\toutput: \n",
      "\tcheckpoint_dir: ../checkpoints_1_tasks\n",
      "\tn_epochs: 100\n",
      "\tbatch_size: 64\n",
      "\tlr: 0.03\n",
      "\thidden_size: 800\n",
      "\tparameter: \n",
      "\tMC_samples: 10\n",
      "\trho: -3.0\n",
      "\tsigma1: 0.0\n",
      "\tsigma2: 6.0\n",
      "\tpi: 0.25\n",
      "\tresume: no\n",
      "\tsti: 0\n",
      "\tfff: /Users/jonastjomsland/Library/Jupyter/runtime/kernel-18c4ed29-0a63-47f1-8751-82cf7f1f276c.json\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Arguments\n",
    "parser=argparse.ArgumentParser(description='xxx')\n",
    "parser.add_argument('--seed',               default=0,              type=int,     help='(default=%(default)d)')\n",
    "parser.add_argument('--device',             default='cpu',          type=str,     help='gpu id')\n",
    "parser.add_argument('--experiment',         default='1_task_groups',       type =str,    help='Mnist or dissertation')\n",
    "parser.add_argument('--approach',           default='PUGCL',          type =str,    help='Method, always Lifelong Uncertainty-aware learning')\n",
    "parser.add_argument('--data_path',          default='data/data.csv',     type=str,     help='gpu id')\n",
    "\n",
    "# Training parameters\n",
    "parser.add_argument('--output',             default='',             type=str,     help='')\n",
    "parser.add_argument('--checkpoint_dir',     default='../checkpoints_1_tasks',    type=str,   help='')\n",
    "parser.add_argument('--n_epochs',           default=100,              type=int,     help='')\n",
    "parser.add_argument('--batch_size',         default=64,             type=int,     help='')\n",
    "parser.add_argument('--lr',                 default=0.03,           type=float,   help='')\n",
    "parser.add_argument('--hidden_size',        default=800,           type=int,     help='')\n",
    "parser.add_argument('--parameter',          default='',             type=str,     help='')\n",
    "\n",
    "# UCB HYPER-PARAMETERS\n",
    "parser.add_argument('--MC_samples',         default='10',           type=int,     help='Number of Monte Carlo samples')\n",
    "parser.add_argument('--rho',                default='-3',           type=float,   help='Initial rho')\n",
    "parser.add_argument('--sigma1',             default='0.0',          type=float,   help='STD foor the 1st prior pdf in scaled mixture Gaussian')\n",
    "parser.add_argument('--sigma2',             default='6.0',          type=float,   help='STD foor the 2nd prior pdf in scaled mixture Gaussian')\n",
    "parser.add_argument('--pi',                 default='0.25',         type=float,   help='weighting factor for prior')\n",
    "\n",
    "parser.add_argument('--resume',             default='no',           type=str,     help='resume?')\n",
    "parser.add_argument('--sti',                default=0,              type=int,     help='starting task?')\n",
    "\n",
    "parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "\n",
    "args=parser.parse_args()\n",
    "utils.print_arguments(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "1_task_groups_PUGCL\n",
      "Results will be saved in  ../checkpoints_1_tasks/1_task_groups_PUGCL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set seed for stable results\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# Check if Cuda is available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"Using device:\", args.device)\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = utils.make_directories(args)\n",
    "args.checkpoint = checkpoint\n",
    "print()\n",
    "\n",
    "# PUGCL with two tasks:\n",
    "from data import dataloader_1_tasks as dataloader\n",
    "\n",
    "# Import Lifelong Uncertainty-aware Learning approach:\n",
    "#from bayesian_model.lul import Lul\n",
    "from training_method import PUGCL\n",
    "\n",
    "# Import model used:\n",
    "#from bayesian_model.bayesian_network import BayesianNetwork\n",
    "from bayesian_model.bayesian_network import BayesianNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting this session on: \n",
      "2020-05-15 16:58\n",
      "Loading data...\n",
      "Input size = [1, 29] \n",
      "Task info = [(0, 16)]\n",
      "Number of data samples:  10000\n",
      "Initializing network...\n",
      "Initialize Lifelong Uncertainty-aware Learning\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonastjomsland/anaconda3/envs/ucb/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Starting this session on: \")\n",
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\n",
    "\n",
    "# Load data:\n",
    "print(\"Loading data...\")\n",
    "data, task_outputs, input_size = dataloader.get(data_path=args.data_path)\n",
    "print(\"Input size =\", input_size, \"\\nTask info =\", task_outputs)\n",
    "print(\"Number of data samples: \", len(data[0]['train']['x']))\n",
    "args.num_tasks = len(task_outputs)\n",
    "args.input_size = input_size\n",
    "args.task_outputs = task_outputs\n",
    "pickle.dump(data, open( \"data/data_1_task.p\", \"wb\" ))\n",
    "\n",
    "# Initialize Bayesian network\n",
    "print(\"Initializing network...\")\n",
    "model = BayesianNetwork(args).to(args.device)\n",
    "\n",
    "# Initialize Lul approach\n",
    "print(\"Initialize Lifelong Uncertainty-aware Learning\")\n",
    "approach = PUGCL(model, args=args)\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Check wether resuming:\n",
    "if args.resume == \"yes\":\n",
    "    checkpoint = torch.load(os.path.join(args.checkpoint, 'model_{}.pth.tar'.format(args.sti)))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device=args.device)\n",
    "else:\n",
    "    args.sti = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Task  0 (All actions)\n",
      "****************************************************************************************************\n",
      "Starting training for the tasks in group:  0\n",
      "Batch: 0/10000 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/python_arg_parser.cpp:750: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 9984/10000 | Epoch   1, time=627.6ms/  9.9ms | Training loss: 1.586 | Learning rate: 0.030 | *\n",
      "Batch: 9984/10000 | Epoch   2, time=608.2ms/  9.4ms | Training loss: 1.492 | Learning rate: 0.030 | *\n",
      "Batch: 9984/10000 | Epoch   3, time=598.6ms/  9.3ms | Training loss: 1.458 | Learning rate: 0.030 | *\n",
      "Batch: 9984/10000 | Epoch   4, time=596.9ms/  9.1ms | Training loss: 1.431 | Learning rate: 0.030 | *\n",
      "Batch: 9984/10000 | Epoch   5, time=596.3ms/  9.2ms | Training loss: 1.411 | Learning rate: 0.030 | *\n",
      "Batch: 9984/10000 | Epoch   6, time=594.3ms/  8.8ms | Training loss: 1.398 | Learning rate: 0.030 | *\n",
      "Batch: 9984/10000 | Epoch   7, time=592.8ms/  9.3ms | Training loss: 1.380 | Learning rate: 0.030 | *\n",
      "Batch: 9984/10000 | Epoch   8, time=588.0ms/  9.1ms | Training loss: 1.386 | Learning rate: 0.030 |\n",
      "Batch: 9984/10000 | Epoch   9, time=585.8ms/  9.1ms | Training loss: 1.374 | Learning rate: 0.030 | *\n",
      "Batch: 9984/10000 | Epoch  10, time=587.0ms/  8.8ms | Training loss: 1.367 | Learning rate: 0.030 | *\n",
      "Batch: 9984/10000 | Epoch  11, time=585.5ms/  8.8ms | Training loss: 1.360 | Learning rate: 0.030 | *\n",
      "Batch: 9984/10000 | Epoch  12, time=579.0ms/  9.1ms | Training loss: 1.363 | Learning rate: 0.030 |\n",
      "Batch: 9984/10000 | Epoch  13, time=574.6ms/  8.8ms | Training loss: 1.349 | Learning rate: 0.030 | *\n",
      "Batch: 9984/10000 | Epoch  14, time=571.3ms/  9.1ms | Training loss: 1.362 | Learning rate: 0.030 |\n",
      "Batch: 9984/10000 | Epoch  15, time=578.4ms/  9.0ms | Training loss: 1.350 | Learning rate: 0.030 |\n",
      "Batch: 9984/10000 | Epoch  16, time=571.8ms/  9.0ms | Training loss: 1.346 | Learning rate: 0.030 | *\n",
      "Batch: 9984/10000 | Epoch  17, time=571.1ms/  9.2ms | Training loss: 1.347 | Learning rate: 0.030 |\n",
      "Batch: 9984/10000 | Epoch  18, time=572.5ms/  8.9ms | Training loss: 1.345 | Learning rate: 0.030 | *\n",
      "Batch: 9984/10000 | Epoch  19, time=571.7ms/  8.7ms | Training loss: 1.341 | Learning rate: 0.030 | *\n",
      "Batch: 9984/10000 | Epoch  20, time=566.8ms/  9.1ms | Training loss: 1.344 | Learning rate: 0.030 |\n",
      "Batch: 9984/10000 | Epoch  21, time=569.9ms/  9.0ms | Training loss: 1.343 | Learning rate: 0.030 |\n",
      "Batch: 9984/10000 | Epoch  22, time=569.1ms/  8.9ms | Training loss: 1.343 | Learning rate: 0.030 |\n",
      "Batch: 9984/10000 | Epoch  23, time=567.9ms/  9.1ms | Training loss: 1.339 | Learning rate: 0.030 | *\n",
      "Batch: 9984/10000 | Epoch  24, time=568.8ms/ 11.4ms | Training loss: 1.333 | Learning rate: 0.030 | *\n",
      "Batch: 9984/10000 | Epoch  25, time=569.8ms/  8.8ms | Training loss: 1.334 | Learning rate: 0.030 |\n",
      "Batch: 9984/10000 | Epoch  26, time=566.7ms/  9.1ms | Training loss: 1.334 | Learning rate: 0.030 |\n",
      "Batch: 9984/10000 | Epoch  27, time=569.1ms/  9.0ms | Training loss: 1.338 | Learning rate: 0.030 |\n",
      "Batch: 9984/10000 | Epoch  28, time=570.9ms/  9.0ms | Training loss: 1.337 | Learning rate: 0.030 |\n",
      "Batch: 9984/10000 | Epoch  29, time=568.4ms/  9.1ms | Training loss: 1.337 | Learning rate: 0.030 | lr=1.0e-02\n",
      "Batch: 9984/10000 | Epoch  30, time=569.7ms/  9.0ms | Training loss: 1.328 | Learning rate: 0.010 | *\n",
      "Batch: 9984/10000 | Epoch  31, time=583.1ms/  9.1ms | Training loss: 1.332 | Learning rate: 0.010 |\n",
      "Batch: 9984/10000 | Epoch  32, time=571.8ms/  8.9ms | Training loss: 1.329 | Learning rate: 0.010 |\n",
      "Batch: 9984/10000 | Epoch  33, time=569.9ms/  9.1ms | Training loss: 1.332 | Learning rate: 0.010 |\n",
      "Batch: 9984/10000 | Epoch  34, time=572.5ms/  9.1ms | Training loss: 1.337 | Learning rate: 0.010 |\n",
      "Batch: 9984/10000 | Epoch  35, time=570.5ms/  9.0ms | Training loss: 1.324 | Learning rate: 0.010 | *\n",
      "Batch: 9984/10000 | Epoch  36, time=569.5ms/  9.1ms | Training loss: 1.332 | Learning rate: 0.010 |\n",
      "Batch: 9984/10000 | Epoch  37, time=569.9ms/ 10.8ms | Training loss: 1.338 | Learning rate: 0.010 |\n",
      "Batch: 9984/10000 | Epoch  38, time=571.2ms/  8.9ms | Training loss: 1.328 | Learning rate: 0.010 |\n",
      "Batch: 9984/10000 | Epoch  39, time=570.1ms/  9.1ms | Training loss: 1.335 | Learning rate: 0.010 |\n",
      "Batch: 9984/10000 | Epoch  40, time=570.1ms/  9.1ms | Training loss: 1.326 | Learning rate: 0.010 | lr=3.3e-03\n",
      "Batch: 9984/10000 | Epoch  41, time=573.9ms/  9.2ms | Training loss: 1.333 | Learning rate: 0.003 |\n",
      "Batch: 9984/10000 | Epoch  42, time=571.6ms/  9.1ms | Training loss: 1.341 | Learning rate: 0.003 |\n",
      "Batch: 9984/10000 | Epoch  43, time=571.0ms/  8.9ms | Training loss: 1.331 | Learning rate: 0.003 |\n",
      "Batch: 9984/10000 | Epoch  44, time=572.3ms/  9.1ms | Training loss: 1.326 | Learning rate: 0.003 |\n",
      "Batch: 9984/10000 | Epoch  45, time=571.3ms/  9.0ms | Training loss: 1.329 | Learning rate: 0.003 | lr=1.1e-03\n",
      "Batch: 9984/10000 | Epoch  46, time=571.2ms/  9.0ms | Training loss: 1.339 | Learning rate: 0.001 |\n",
      "Batch: 9984/10000 | Epoch  47, time=573.9ms/  8.9ms | Training loss: 1.327 | Learning rate: 0.001 |\n",
      "Batch: 9984/10000 | Epoch  48, time=572.0ms/  8.9ms | Training loss: 1.333 | Learning rate: 0.001 |\n",
      "Batch: 9984/10000 | Epoch  49, time=571.1ms/  9.1ms | Training loss: 1.328 | Learning rate: 0.001 |\n",
      "Batch: 9984/10000 | Epoch  50, time=571.0ms/  9.0ms | Training loss: 1.332 | Learning rate: 0.001 | lr=3.7e-04\n",
      "Batch: 9984/10000 | Epoch  51, time=571.2ms/  8.9ms | Training loss: 1.334 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  52, time=570.7ms/  9.2ms | Training loss: 1.330 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  53, time=570.9ms/  9.0ms | Training loss: 1.333 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  54, time=574.7ms/  9.0ms | Training loss: 1.331 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  55, time=570.9ms/  9.2ms | Training loss: 1.330 | Learning rate: 0.000 | lr=1.2e-04\n",
      "Batch: 9984/10000 | Epoch  56, time=570.2ms/  9.1ms | Training loss: 1.327 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  57, time=572.3ms/  8.8ms | Training loss: 1.328 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  58, time=572.2ms/  9.0ms | Training loss: 1.329 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  59, time=569.9ms/  9.0ms | Training loss: 1.331 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  60, time=572.5ms/  8.9ms | Training loss: 1.332 | Learning rate: 0.000 | lr=4.1e-05\n",
      "Batch: 9984/10000 | Epoch  61, time=572.4ms/  8.9ms | Training loss: 1.335 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  62, time=569.5ms/  9.1ms | Training loss: 1.338 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  63, time=571.3ms/  9.0ms | Training loss: 1.336 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  64, time=573.3ms/  8.9ms | Training loss: 1.339 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  65, time=570.5ms/  9.2ms | Training loss: 1.334 | Learning rate: 0.000 | lr=1.4e-05\n",
      "Batch: 9984/10000 | Epoch  66, time=570.7ms/  9.0ms | Training loss: 1.336 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  67, time=577.5ms/  9.1ms | Training loss: 1.333 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  68, time=570.0ms/  9.2ms | Training loss: 1.337 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  69, time=568.6ms/  9.0ms | Training loss: 1.334 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  70, time=571.6ms/  9.1ms | Training loss: 1.333 | Learning rate: 0.000 | lr=4.6e-06\n",
      "Batch: 9984/10000 | Epoch  71, time=571.8ms/  8.9ms | Training loss: 1.340 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  72, time=571.9ms/  9.0ms | Training loss: 1.337 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  73, time=580.5ms/  9.0ms | Training loss: 1.336 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  74, time=573.5ms/  9.2ms | Training loss: 1.334 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  75, time=571.6ms/  9.1ms | Training loss: 1.334 | Learning rate: 0.000 | lr=1.5e-06\n",
      "Batch: 9984/10000 | Epoch  76, time=572.1ms/  9.2ms | Training loss: 1.336 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  77, time=573.3ms/  9.0ms | Training loss: 1.339 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  78, time=573.1ms/  8.9ms | Training loss: 1.338 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  79, time=573.4ms/  9.2ms | Training loss: 1.348 | Learning rate: 0.000 |\n",
      "Batch: 9984/10000 | Epoch  80, time=575.0ms/  8.9ms | Training loss: 1.341 | Learning rate: 0.000 | lr=5.1e-07\n",
      "____________________________________________________________________________________________________\n",
      "Test on task  0 - All actions    : loss=1.335\n",
      "Saving at ../checkpoints_1_tasks/1_task_groups_PUGCL\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the tasks:\n",
    "loss = np.zeros((len(task_outputs), len(task_outputs)), dtype=np.float32)\n",
    "for task, n_class in task_outputs[args.sti:]:\n",
    "    print('*'*100)\n",
    "    print('Task {:2d} ({:s})'.format(task, data[task]['name']))\n",
    "    print('*'*100)\n",
    "\n",
    "    # Get data:\n",
    "    xtrain = data[task]['train']['x'][:,1:].type(torch.float32).to(args.device)\n",
    "    ytrain = data[task]['train']['y'].type(torch.float32).to(args.device)\n",
    "    xvalid = data[task]['valid']['x'][:,1:].type(torch.float32).to(args.device)\n",
    "    yvalid = data[task]['valid']['y'].type(torch.float32).to(args.device)\n",
    "\n",
    "    # Start training\n",
    "    print(\"Starting training for the tasks in group: \", task)\n",
    "    approach.train(task, xtrain, ytrain, xvalid, yvalid)\n",
    "    print('_'*100)\n",
    "\n",
    "    # Validate for this task group:\n",
    "    for u in range(task+1):\n",
    "        xtest = data[u]['test']['x'][:,1:].type(torch.float32).to(args.device)\n",
    "        ytest = data[u]['test']['y'].type(torch.float32).to(args.device)\n",
    "        test_loss = approach.eval(u, xtest, ytest, debug=True)\n",
    "        print(\"Test on task {:2d} - {:15s}: loss={:.3f}\".format(u, data[u]['name'], test_loss))\n",
    "        loss[task, u] = test_loss\n",
    "\n",
    "    # Save\n",
    "    print(\"Saving at \" + args.checkpoint)\n",
    "    np.savetxt(os.path.join(args.checkpoint, '{}_{}_{}.txt'.format(args.experiment, args.approach, args.seed)), loss, '%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
