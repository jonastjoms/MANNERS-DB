{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,argparse,time\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "tstart=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Arguments =\n",
      "\tseed: 0\n",
      "\tdevice: cpu\n",
      "\texperiment: 16_task_groups\n",
      "\tapproach: PUGCL\n",
      "\tdata_path: data/data.csv\n",
      "\toutput: \n",
      "\tcheckpoint_dir: ../checkpoints_16_tasks\n",
      "\tn_epochs: 100\n",
      "\tbatch_size: 64\n",
      "\tlr: 0.03\n",
      "\thidden_size: 800\n",
      "\tparameter: \n",
      "\tMC_samples: 10\n",
      "\trho: -3.0\n",
      "\tsigma1: 0.0\n",
      "\tsigma2: 6.0\n",
      "\tpi: 0.25\n",
      "\tresume: no\n",
      "\tsti: 1\n",
      "\tfff: /Users/jonastjomsland/Library/Jupyter/runtime/kernel-003dd4ae-5b1e-4b75-9dcd-182fa5aa8112.json\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Arguments\n",
    "parser=argparse.ArgumentParser(description='xxx')\n",
    "parser.add_argument('--seed',               default=0,              type=int,     help='(default=%(default)d)')\n",
    "parser.add_argument('--device',             default='cpu',          type=str,     help='gpu id')\n",
    "parser.add_argument('--experiment',         default='16_task_groups',       type =str,    help='Mnist or dissertation')\n",
    "parser.add_argument('--approach',           default='PUGCL',          type =str,    help='Method, always Lifelong Uncertainty-aware learning')\n",
    "parser.add_argument('--data_path',          default='data/data.csv',     type=str,     help='gpu id')\n",
    "\n",
    "# Training parameters\n",
    "parser.add_argument('--output',             default='',             type=str,     help='')\n",
    "parser.add_argument('--checkpoint_dir',     default='../checkpoints_16_tasks',    type=str,   help='')\n",
    "parser.add_argument('--n_epochs',           default=100,              type=int,     help='')\n",
    "parser.add_argument('--batch_size',         default=64,             type=int,     help='')\n",
    "parser.add_argument('--lr',                 default=0.03,           type=float,   help='')\n",
    "parser.add_argument('--hidden_size',        default=800,           type=int,     help='')\n",
    "parser.add_argument('--parameter',          default='',             type=str,     help='')\n",
    "\n",
    "# UCB HYPER-PARAMETERS\n",
    "parser.add_argument('--MC_samples',         default='10',           type=int,     help='Number of Monte Carlo samples')\n",
    "parser.add_argument('--rho',                default='-3',           type=float,   help='Initial rho')\n",
    "parser.add_argument('--sigma1',             default='0.0',          type=float,   help='STD foor the 1st prior pdf in scaled mixture Gaussian')\n",
    "parser.add_argument('--sigma2',             default='6.0',          type=float,   help='STD foor the 2nd prior pdf in scaled mixture Gaussian')\n",
    "parser.add_argument('--pi',                 default='0.25',         type=float,   help='weighting factor for prior')\n",
    "\n",
    "parser.add_argument('--resume',             default='no',           type=str,     help='resume?')\n",
    "parser.add_argument('--sti',                default=1,              type=int,     help='starting task?')\n",
    "\n",
    "parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "\n",
    "args=parser.parse_args()\n",
    "utils.print_arguments(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "16_task_groups_PUGCL\n",
      "Results will be saved in  ../checkpoints_16_tasks/16_task_groups_PUGCL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set seed for stable results\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# Check if Cuda is available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"Using device:\", args.device)\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = utils.make_directories(args)\n",
    "args.checkpoint = checkpoint\n",
    "print()\n",
    "\n",
    "# PUGCL with two tasks:\n",
    "from data import dataloader_16_tasks as dataloader\n",
    "\n",
    "# Import Lifelong Uncertainty-aware Learning approach:\n",
    "#from bayesian_model.lul import Lul\n",
    "from training_method import PUGCL\n",
    "\n",
    "# Import model used:\n",
    "#from bayesian_model.bayesian_network import BayesianNetwork\n",
    "from bayesian_model.bayesian_network import BayesianNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting this session on: \n",
      "2020-05-15 10:33\n",
      "Loading data...\n",
      "Input size = [1, 29] \n",
      "Task info = [(0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (5, 2), (6, 2), (7, 2), (8, 2), (9, 2), (10, 2), (11, 2), (12, 2), (13, 2), (14, 2), (15, 2)]\n",
      "Number of data samples:  500\n",
      "Initializing network...\n",
      "Initialize Lifelong Uncertainty-aware Learning\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonastjomsland/anaconda3/envs/ucb/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Starting this session on: \")\n",
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\n",
    "\n",
    "# Load data:\n",
    "print(\"Loading data...\")\n",
    "data, task_outputs, input_size = dataloader.get(data_path=args.data_path)\n",
    "print(\"Input size =\", input_size, \"\\nTask info =\", task_outputs)\n",
    "print(\"Number of data samples: \", len(data[0]['train']['x']))\n",
    "args.num_tasks = len(task_outputs)\n",
    "args.input_size = input_size\n",
    "args.task_outputs = task_outputs\n",
    "pickle.dump(data, open( \"data/data.p\", \"wb\" ))\n",
    "\n",
    "# Initialize Bayesian network\n",
    "print(\"Initializing network...\")\n",
    "model = BayesianNetwork(args).to(args.device)\n",
    "\n",
    "# Initialize Lul approach\n",
    "print(\"Initialize Lifelong Uncertainty-aware Learning\")\n",
    "approach = PUGCL(model, args=args)\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Check wether resuming:\n",
    "if args.resume == \"yes\":\n",
    "    checkpoint = torch.load(os.path.join(args.checkpoint, 'model_{}.pth.tar'.format(args.sti)))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device=args.device)\n",
    "else:\n",
    "    args.sti = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Task  0 (Vacuum cleaning)\n",
      "****************************************************************************************************\n",
      "Starting training for the tasks in group:  0\n",
      "Batch: 0/500 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/python_arg_parser.cpp:750: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 448/500 | Epoch   1, time=626.5ms/ 12.3ms | Training loss: 1.845 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch   2, time=619.5ms/ 12.2ms | Training loss: 1.755 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch   3, time=590.6ms/ 12.1ms | Training loss: 1.711 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch   4, time=590.7ms/ 12.0ms | Training loss: 1.690 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch   5, time=590.3ms/ 13.9ms | Training loss: 1.641 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch   6, time=613.7ms/ 12.0ms | Training loss: 1.748 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch   7, time=589.4ms/ 12.1ms | Training loss: 1.702 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch   8, time=587.6ms/ 12.3ms | Training loss: 1.697 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch   9, time=587.1ms/ 12.0ms | Training loss: 1.585 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  10, time=591.2ms/ 12.2ms | Training loss: 1.700 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  11, time=586.5ms/ 12.5ms | Training loss: 1.584 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  12, time=586.9ms/ 12.2ms | Training loss: 1.543 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  13, time=585.6ms/ 12.3ms | Training loss: 1.610 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  14, time=585.8ms/ 12.1ms | Training loss: 1.534 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  15, time=586.1ms/ 11.9ms | Training loss: 1.525 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  16, time=590.3ms/ 12.4ms | Training loss: 1.518 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  17, time=586.5ms/ 12.0ms | Training loss: 1.510 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  18, time=586.0ms/ 12.2ms | Training loss: 1.506 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  19, time=587.7ms/ 12.2ms | Training loss: 1.618 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  20, time=589.3ms/ 12.2ms | Training loss: 1.509 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  21, time=589.5ms/ 13.2ms | Training loss: 1.559 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  22, time=587.3ms/ 12.2ms | Training loss: 1.510 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  23, time=601.1ms/ 13.1ms | Training loss: 1.554 | Learning rate: 0.030 | lr=1.0e-02\n",
      "Batch: 448/500 | Epoch  24, time=589.5ms/ 12.2ms | Training loss: 1.488 | Learning rate: 0.010 | *\n",
      "Batch: 448/500 | Epoch  25, time=592.0ms/ 13.0ms | Training loss: 1.531 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  26, time=591.3ms/ 12.0ms | Training loss: 1.542 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  27, time=610.3ms/ 13.1ms | Training loss: 1.555 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  28, time=600.6ms/ 12.8ms | Training loss: 1.489 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  29, time=601.6ms/ 12.0ms | Training loss: 1.478 | Learning rate: 0.010 | *\n",
      "Batch: 448/500 | Epoch  30, time=595.8ms/ 12.6ms | Training loss: 1.489 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  31, time=596.6ms/ 12.3ms | Training loss: 1.486 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  32, time=617.5ms/ 15.0ms | Training loss: 1.571 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  33, time=652.0ms/ 14.9ms | Training loss: 1.494 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  34, time=624.6ms/ 14.1ms | Training loss: 1.551 | Learning rate: 0.010 | lr=3.3e-03\n",
      "Batch: 448/500 | Epoch  35, time=614.1ms/ 12.7ms | Training loss: 1.484 | Learning rate: 0.003 |\n",
      "Batch: 448/500 | Epoch  36, time=611.2ms/ 14.3ms | Training loss: 1.474 | Learning rate: 0.003 | *\n",
      "Batch: 448/500 | Epoch  37, time=607.3ms/ 13.9ms | Training loss: 1.562 | Learning rate: 0.003 |\n",
      "Batch: 448/500 | Epoch  38, time=595.1ms/ 12.8ms | Training loss: 1.478 | Learning rate: 0.003 |\n",
      "Batch: 448/500 | Epoch  39, time=590.8ms/ 12.7ms | Training loss: 1.591 | Learning rate: 0.003 |\n",
      "Batch: 448/500 | Epoch  40, time=591.8ms/ 12.7ms | Training loss: 1.492 | Learning rate: 0.003 |\n",
      "Batch: 448/500 | Epoch  41, time=591.7ms/ 12.2ms | Training loss: 1.530 | Learning rate: 0.003 | lr=1.1e-03\n",
      "Batch: 448/500 | Epoch  42, time=593.0ms/ 12.4ms | Training loss: 1.484 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  43, time=594.8ms/ 12.4ms | Training loss: 1.469 | Learning rate: 0.001 | *\n",
      "Batch: 448/500 | Epoch  44, time=595.9ms/ 12.9ms | Training loss: 1.512 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  45, time=591.5ms/ 12.5ms | Training loss: 1.508 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  46, time=595.6ms/ 12.0ms | Training loss: 1.537 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  47, time=594.1ms/ 12.3ms | Training loss: 1.459 | Learning rate: 0.001 | *\n",
      "Batch: 448/500 | Epoch  48, time=592.0ms/ 12.3ms | Training loss: 1.505 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  49, time=593.5ms/ 12.1ms | Training loss: 1.498 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  50, time=593.9ms/ 12.1ms | Training loss: 1.534 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  51, time=621.0ms/ 12.1ms | Training loss: 1.500 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  52, time=602.3ms/ 12.2ms | Training loss: 1.459 | Learning rate: 0.001 | lr=3.7e-04\n",
      "Batch: 448/500 | Epoch  53, time=596.3ms/ 13.1ms | Training loss: 1.496 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  54, time=591.5ms/ 12.1ms | Training loss: 1.564 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  55, time=592.2ms/ 12.2ms | Training loss: 1.503 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  56, time=592.9ms/ 12.3ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  57, time=597.2ms/ 12.8ms | Training loss: 1.475 | Learning rate: 0.000 | lr=1.2e-04\n",
      "Batch: 448/500 | Epoch  58, time=611.4ms/ 14.2ms | Training loss: 1.471 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  59, time=600.2ms/ 14.1ms | Training loss: 1.560 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  60, time=607.1ms/ 12.8ms | Training loss: 1.455 | Learning rate: 0.000 | *\n",
      "Batch: 448/500 | Epoch  61, time=613.0ms/ 12.2ms | Training loss: 1.450 | Learning rate: 0.000 | *\n",
      "Batch: 448/500 | Epoch  62, time=612.4ms/ 12.2ms | Training loss: 1.506 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  63, time=596.2ms/ 15.8ms | Training loss: 1.452 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  64, time=607.5ms/ 12.4ms | Training loss: 1.452 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  65, time=612.9ms/ 14.1ms | Training loss: 1.466 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  66, time=613.6ms/ 13.2ms | Training loss: 1.493 | Learning rate: 0.000 | lr=4.1e-05\n",
      "Batch: 448/500 | Epoch  67, time=622.9ms/ 12.0ms | Training loss: 1.470 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  68, time=607.8ms/ 14.2ms | Training loss: 1.457 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  69, time=599.0ms/ 12.2ms | Training loss: 1.488 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  70, time=585.3ms/ 12.2ms | Training loss: 1.447 | Learning rate: 0.000 | *\n",
      "Batch: 448/500 | Epoch  71, time=610.8ms/ 13.1ms | Training loss: 1.553 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  72, time=607.1ms/ 12.2ms | Training loss: 1.448 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  73, time=612.6ms/ 12.6ms | Training loss: 1.525 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  74, time=601.9ms/ 12.6ms | Training loss: 1.476 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  75, time=592.8ms/ 12.4ms | Training loss: 1.475 | Learning rate: 0.000 | lr=1.4e-05\n",
      "Batch: 448/500 | Epoch  76, time=604.6ms/ 15.4ms | Training loss: 1.480 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  77, time=623.0ms/ 13.3ms | Training loss: 1.454 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  78, time=603.1ms/ 12.7ms | Training loss: 1.498 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  79, time=592.4ms/ 12.4ms | Training loss: 1.459 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  80, time=594.9ms/ 12.6ms | Training loss: 1.457 | Learning rate: 0.000 | lr=4.6e-06\n",
      "Batch: 448/500 | Epoch  81, time=610.1ms/ 14.1ms | Training loss: 1.447 | Learning rate: 0.000 | *\n",
      "Batch: 448/500 | Epoch  82, time=605.0ms/ 12.6ms | Training loss: 1.451 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  83, time=610.8ms/ 13.5ms | Training loss: 1.489 | Learning rate: 0.000 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 448/500 | Epoch  84, time=615.7ms/ 14.0ms | Training loss: 1.544 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  85, time=611.1ms/ 12.3ms | Training loss: 1.481 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  86, time=606.9ms/ 14.0ms | Training loss: 1.441 | Learning rate: 0.000 | *\n",
      "Batch: 448/500 | Epoch  87, time=606.8ms/ 14.9ms | Training loss: 1.441 | Learning rate: 0.000 | *\n",
      "Batch: 448/500 | Epoch  88, time=613.8ms/ 12.5ms | Training loss: 1.454 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  89, time=598.7ms/ 15.4ms | Training loss: 1.477 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  90, time=611.1ms/ 13.9ms | Training loss: 1.478 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  91, time=602.5ms/ 14.1ms | Training loss: 1.441 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  92, time=614.2ms/ 13.0ms | Training loss: 1.500 | Learning rate: 0.000 | lr=1.5e-06\n",
      "Batch: 448/500 | Epoch  93, time=626.0ms/ 13.1ms | Training loss: 1.456 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  94, time=615.0ms/ 13.6ms | Training loss: 1.492 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  95, time=599.5ms/ 13.3ms | Training loss: 1.442 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  96, time=603.1ms/ 12.9ms | Training loss: 1.484 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  97, time=591.7ms/ 12.5ms | Training loss: 1.490 | Learning rate: 0.000 | lr=5.1e-07\n",
      "____________________________________________________________________________________________________\n",
      "Test on task  0 - Vacuum cleaning: loss=1.494\n",
      "Saving at ../checkpoints_16_tasks/16_task_groups_PUGCL\n",
      "****************************************************************************************************\n",
      "Task  1 (Mopping the floor)\n",
      "****************************************************************************************************\n",
      "Starting training for the tasks in group:  1\n",
      "Batch: 448/500 | Epoch   1, time=601.4ms/ 13.5ms | Training loss: 2.105 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   2, time=615.0ms/ 12.4ms | Training loss: 1.957 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   3, time=586.2ms/ 14.3ms | Training loss: 1.647 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   4, time=610.1ms/ 13.5ms | Training loss: 1.577 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   5, time=594.3ms/ 12.1ms | Training loss: 1.628 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch   6, time=585.5ms/ 12.5ms | Training loss: 1.592 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch   7, time=617.6ms/ 13.5ms | Training loss: 1.529 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   8, time=613.6ms/ 14.9ms | Training loss: 1.670 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch   9, time=616.9ms/ 13.6ms | Training loss: 1.740 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  10, time=607.3ms/ 13.9ms | Training loss: 1.652 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  11, time=602.6ms/ 12.0ms | Training loss: 1.587 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  12, time=604.4ms/ 13.8ms | Training loss: 1.743 | Learning rate: 0.050 | lr=1.7e-02\n",
      "Batch: 448/500 | Epoch  13, time=625.4ms/ 14.8ms | Training loss: 1.743 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  14, time=610.3ms/ 14.2ms | Training loss: 1.743 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  15, time=610.1ms/ 13.0ms | Training loss: 1.743 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  16, time=604.2ms/ 14.5ms | Training loss: 1.743 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  17, time=611.2ms/ 12.8ms | Training loss: 1.743 | Learning rate: 0.017 | lr=5.6e-03\n",
      "Batch: 448/500 | Epoch  18, time=597.0ms/ 12.0ms | Training loss: 1.743 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  19, time=589.3ms/ 12.6ms | Training loss: 1.743 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  20, time=600.0ms/ 13.8ms | Training loss: 1.743 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  21, time=614.6ms/ 14.1ms | Training loss: 1.743 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  22, time=601.4ms/ 12.0ms | Training loss: 1.743 | Learning rate: 0.006 | lr=1.9e-03\n",
      "Batch: 448/500 | Epoch  23, time=586.7ms/ 12.5ms | Training loss: 1.743 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  24, time=587.7ms/ 12.2ms | Training loss: 1.743 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  25, time=588.2ms/ 12.7ms | Training loss: 1.743 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  26, time=591.4ms/ 12.1ms | Training loss: 1.743 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  27, time=604.7ms/ 13.6ms | Training loss: 1.743 | Learning rate: 0.002 | lr=6.2e-04\n",
      "Batch: 448/500 | Epoch  28, time=586.8ms/ 12.1ms | Training loss: 1.743 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  29, time=588.3ms/ 12.4ms | Training loss: 1.743 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  30, time=585.8ms/ 12.2ms | Training loss: 1.743 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  31, time=586.4ms/ 12.1ms | Training loss: 1.743 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  32, time=589.0ms/ 12.2ms | Training loss: 1.743 | Learning rate: 0.001 | lr=2.1e-04\n",
      "Batch: 448/500 | Epoch  33, time=610.0ms/ 13.2ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  34, time=636.2ms/ 13.2ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  35, time=619.9ms/ 13.8ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  36, time=616.0ms/ 14.8ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  37, time=611.6ms/ 13.1ms | Training loss: 1.743 | Learning rate: 0.000 | lr=6.9e-05\n",
      "Batch: 448/500 | Epoch  38, time=606.6ms/ 12.5ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  39, time=601.5ms/ 12.7ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  40, time=610.1ms/ 12.4ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  41, time=625.8ms/ 15.9ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  42, time=623.2ms/ 13.9ms | Training loss: 1.743 | Learning rate: 0.000 | lr=2.3e-05\n",
      "Batch: 448/500 | Epoch  43, time=619.1ms/ 12.4ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  44, time=622.6ms/ 14.0ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  45, time=611.9ms/ 13.5ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  46, time=604.0ms/ 12.7ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  47, time=619.5ms/ 13.5ms | Training loss: 1.743 | Learning rate: 0.000 | lr=7.6e-06\n",
      "Batch: 448/500 | Epoch  48, time=616.4ms/ 12.2ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  49, time=594.2ms/ 12.2ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  50, time=600.3ms/ 14.2ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  51, time=621.2ms/ 12.3ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  52, time=608.7ms/ 12.3ms | Training loss: 1.743 | Learning rate: 0.000 | lr=2.5e-06\n",
      "Batch: 448/500 | Epoch  53, time=609.4ms/ 17.5ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  54, time=615.8ms/ 14.3ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  55, time=616.5ms/ 12.2ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  56, time=616.4ms/ 12.5ms | Training loss: 1.743 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  57, time=612.5ms/ 13.2ms | Training loss: 1.743 | Learning rate: 0.000 | lr=8.5e-07\n",
      "____________________________________________________________________________________________________\n",
      "Test on task  0 - Vacuum cleaning: loss=1.494\n",
      "Test on task  1 - Mopping the floor: loss=1.546\n",
      "Saving at ../checkpoints_16_tasks/16_task_groups_PUGCL\n",
      "****************************************************************************************************\n",
      "Task  2 (Carry warm food)\n",
      "****************************************************************************************************\n",
      "Starting training for the tasks in group:  2\n",
      "Batch: 448/500 | Epoch   1, time=595.9ms/ 12.3ms | Training loss: 1.759 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   2, time=589.0ms/ 11.9ms | Training loss: 1.489 | Learning rate: 0.050 | *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 448/500 | Epoch   3, time=588.4ms/ 12.3ms | Training loss: 1.486 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   4, time=622.6ms/ 12.7ms | Training loss: 1.449 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   5, time=613.7ms/ 13.1ms | Training loss: 1.475 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch   6, time=605.1ms/ 12.0ms | Training loss: 1.499 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch   7, time=588.9ms/ 12.4ms | Training loss: 1.494 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch   8, time=615.1ms/ 13.1ms | Training loss: 1.426 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   9, time=604.3ms/ 16.1ms | Training loss: 1.530 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  10, time=619.4ms/ 12.8ms | Training loss: 1.395 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  11, time=603.8ms/ 12.4ms | Training loss: 1.390 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  12, time=595.2ms/ 12.8ms | Training loss: 1.422 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  13, time=595.3ms/ 12.3ms | Training loss: 1.390 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  14, time=615.3ms/ 14.6ms | Training loss: 1.452 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  15, time=600.8ms/ 12.4ms | Training loss: 1.411 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  16, time=603.0ms/ 13.6ms | Training loss: 1.380 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  17, time=621.2ms/ 12.8ms | Training loss: 1.520 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  18, time=606.7ms/ 13.4ms | Training loss: 1.362 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  19, time=613.6ms/ 12.8ms | Training loss: 1.438 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  20, time=595.3ms/ 12.0ms | Training loss: 1.375 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  21, time=612.6ms/ 13.2ms | Training loss: 1.418 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  22, time=623.4ms/ 12.7ms | Training loss: 1.351 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  23, time=631.5ms/ 15.8ms | Training loss: 1.349 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  24, time=609.9ms/ 12.7ms | Training loss: 1.353 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  25, time=612.8ms/ 14.5ms | Training loss: 1.401 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  26, time=613.7ms/ 14.6ms | Training loss: 1.468 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  27, time=618.7ms/ 12.7ms | Training loss: 1.342 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  28, time=621.3ms/ 13.3ms | Training loss: 1.432 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  29, time=615.2ms/ 12.8ms | Training loss: 1.376 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  30, time=624.5ms/ 14.2ms | Training loss: 1.355 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  31, time=633.2ms/ 13.3ms | Training loss: 1.381 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  32, time=612.4ms/ 12.9ms | Training loss: 1.346 | Learning rate: 0.050 | lr=1.7e-02\n",
      "Batch: 448/500 | Epoch  33, time=608.6ms/ 14.6ms | Training loss: 1.346 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  34, time=616.9ms/ 14.7ms | Training loss: 1.346 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  35, time=608.0ms/ 15.3ms | Training loss: 1.346 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  36, time=622.6ms/ 13.7ms | Training loss: 1.346 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  37, time=599.1ms/ 12.1ms | Training loss: 1.346 | Learning rate: 0.017 | lr=5.6e-03\n",
      "Batch: 448/500 | Epoch  38, time=600.2ms/ 13.1ms | Training loss: 1.346 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  39, time=602.7ms/ 13.9ms | Training loss: 1.346 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  40, time=618.0ms/ 14.1ms | Training loss: 1.346 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  41, time=598.3ms/ 13.9ms | Training loss: 1.346 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  42, time=609.1ms/ 13.4ms | Training loss: 1.346 | Learning rate: 0.006 | lr=1.9e-03\n",
      "Batch: 448/500 | Epoch  43, time=602.2ms/ 13.1ms | Training loss: 1.346 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  44, time=603.6ms/ 12.8ms | Training loss: 1.346 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  45, time=607.0ms/ 12.2ms | Training loss: 1.346 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  46, time=590.0ms/ 12.3ms | Training loss: 1.346 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  47, time=586.8ms/ 12.2ms | Training loss: 1.346 | Learning rate: 0.002 | lr=6.2e-04\n",
      "Batch: 448/500 | Epoch  48, time=587.3ms/ 12.2ms | Training loss: 1.346 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  49, time=587.2ms/ 12.5ms | Training loss: 1.346 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  50, time=586.2ms/ 12.4ms | Training loss: 1.346 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  51, time=588.7ms/ 12.8ms | Training loss: 1.346 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  52, time=586.2ms/ 12.6ms | Training loss: 1.346 | Learning rate: 0.001 | lr=2.1e-04\n",
      "Batch: 448/500 | Epoch  53, time=586.2ms/ 12.0ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  54, time=585.8ms/ 12.3ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  55, time=587.8ms/ 12.1ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  56, time=586.6ms/ 12.2ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  57, time=587.3ms/ 12.1ms | Training loss: 1.346 | Learning rate: 0.000 | lr=6.9e-05\n",
      "Batch: 448/500 | Epoch  58, time=586.5ms/ 12.6ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  59, time=586.1ms/ 12.6ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  60, time=600.4ms/ 12.3ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  61, time=588.5ms/ 12.2ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  62, time=588.3ms/ 12.0ms | Training loss: 1.346 | Learning rate: 0.000 | lr=2.3e-05\n",
      "Batch: 448/500 | Epoch  63, time=603.1ms/ 18.0ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  64, time=725.0ms/ 46.7ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  65, time=632.0ms/ 14.9ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  66, time=595.0ms/ 12.4ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  67, time=592.6ms/ 12.6ms | Training loss: 1.346 | Learning rate: 0.000 | lr=7.6e-06\n",
      "Batch: 448/500 | Epoch  68, time=593.2ms/ 12.1ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  69, time=596.8ms/ 12.4ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  70, time=594.1ms/ 12.2ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  71, time=597.5ms/ 12.4ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  72, time=594.4ms/ 12.4ms | Training loss: 1.346 | Learning rate: 0.000 | lr=2.5e-06\n",
      "Batch: 448/500 | Epoch  73, time=594.5ms/ 12.0ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  74, time=596.2ms/ 12.4ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  75, time=646.4ms/ 14.9ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  76, time=613.6ms/ 12.7ms | Training loss: 1.346 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  77, time=624.7ms/ 12.5ms | Training loss: 1.346 | Learning rate: 0.000 | lr=8.5e-07\n",
      "____________________________________________________________________________________________________\n",
      "Test on task  0 - Vacuum cleaning: loss=1.495\n",
      "Test on task  1 - Mopping the floor: loss=1.649\n",
      "Test on task  2 - Carry warm food: loss=1.323\n",
      "Saving at ../checkpoints_16_tasks/16_task_groups_PUGCL\n",
      "****************************************************************************************************\n",
      "Task  3 (Carry cold food)\n",
      "****************************************************************************************************\n",
      "Starting training for the tasks in group:  3\n",
      "Batch: 448/500 | Epoch   1, time=633.8ms/ 14.1ms | Training loss: 1.475 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   2, time=632.4ms/ 12.1ms | Training loss: 1.422 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   3, time=624.9ms/ 12.1ms | Training loss: 1.436 | Learning rate: 0.050 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 448/500 | Epoch   4, time=608.0ms/ 14.4ms | Training loss: 1.403 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   5, time=602.7ms/ 13.0ms | Training loss: 1.390 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   6, time=625.1ms/ 13.8ms | Training loss: 1.379 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   7, time=603.8ms/ 12.1ms | Training loss: 1.346 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   8, time=608.7ms/ 14.1ms | Training loss: 1.337 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   9, time=626.1ms/ 14.4ms | Training loss: 1.328 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  10, time=620.8ms/ 12.6ms | Training loss: 1.342 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  11, time=621.3ms/ 13.1ms | Training loss: 1.358 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  12, time=632.8ms/ 32.0ms | Training loss: 1.327 | Learning rate: 0.050 | *\n",
      "Batch: 256/500 "
     ]
    }
   ],
   "source": [
    "# Iterate over the two tasks:\n",
    "loss = np.zeros((len(task_outputs), len(task_outputs)), dtype=np.float32)\n",
    "for task, n_class in task_outputs[args.sti:]:\n",
    "    print('*'*100)\n",
    "    print('Task {:2d} ({:s})'.format(task, data[task]['name']))\n",
    "    print('*'*100)\n",
    "\n",
    "    # Get data:\n",
    "    xtrain = data[task]['train']['x'][:,1:].type(torch.float32).to(args.device)\n",
    "    ytrain = data[task]['train']['y'].type(torch.float32).to(args.device)\n",
    "    xvalid = data[task]['valid']['x'][:,1:].type(torch.float32).to(args.device)\n",
    "    yvalid = data[task]['valid']['y'].type(torch.float32).to(args.device)\n",
    "\n",
    "    # Start training\n",
    "    print(\"Starting training for the tasks in group: \", task)\n",
    "    approach.train(task, xtrain, ytrain, xvalid, yvalid)\n",
    "    print('_'*100)\n",
    "\n",
    "    # Validate for this task group:\n",
    "    for u in range(task+1):\n",
    "        xtest = data[u]['test']['x'][:,1:].type(torch.float32).to(args.device)\n",
    "        ytest = data[u]['test']['y'].type(torch.float32).to(args.device)\n",
    "        test_loss = approach.eval(u, xtest, ytest, debug=True)\n",
    "        print(\"Test on task {:2d} - {:15s}: loss={:.3f}\".format(u, data[u]['name'], test_loss))\n",
    "        loss[task, u] = test_loss\n",
    "\n",
    "    # Save\n",
    "    print(\"Saving at \" + args.checkpoint)\n",
    "    np.savetxt(os.path.join(args.checkpoint, '{}_{}_{}.txt'.format(args.experiment, args.approach, args.seed)), loss, '%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
