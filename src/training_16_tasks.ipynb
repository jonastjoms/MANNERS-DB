{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,argparse,time\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "tstart=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Arguments =\n",
      "\tseed: 0\n",
      "\tdevice: cpu\n",
      "\texperiment: 16_task_groups\n",
      "\tapproach: PUGCL\n",
      "\tdata_path: data/data.csv\n",
      "\toutput: \n",
      "\tcheckpoint_dir: ../checkpoints_16_tasks\n",
      "\tn_epochs: 100\n",
      "\tbatch_size: 64\n",
      "\tlr: 0.001\n",
      "\thidden_size: 800\n",
      "\tparameter: \n",
      "\tMC_samples: 10\n",
      "\trho: -3.0\n",
      "\tsigma1: 0.0\n",
      "\tsigma2: 6.0\n",
      "\tpi: 0.25\n",
      "\tresume: no\n",
      "\tsti: 1\n",
      "\tfff: /Users/jonastjomsland/Library/Jupyter/runtime/kernel-003dd4ae-5b1e-4b75-9dcd-182fa5aa8112.json\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Arguments\n",
    "parser=argparse.ArgumentParser(description='xxx')\n",
    "parser.add_argument('--seed',               default=0,              type=int,     help='(default=%(default)d)')\n",
    "parser.add_argument('--device',             default='cpu',          type=str,     help='gpu id')\n",
    "parser.add_argument('--experiment',         default='16_task_groups',       type =str,    help='Mnist or dissertation')\n",
    "parser.add_argument('--approach',           default='PUGCL',          type =str,    help='Method, always Lifelong Uncertainty-aware learning')\n",
    "parser.add_argument('--data_path',          default='data/data.csv',     type=str,     help='gpu id')\n",
    "\n",
    "# Training parameters\n",
    "parser.add_argument('--output',             default='',             type=str,     help='')\n",
    "parser.add_argument('--checkpoint_dir',     default='../checkpoints_16_tasks',    type=str,   help='')\n",
    "parser.add_argument('--n_epochs',           default=100,              type=int,     help='')\n",
    "parser.add_argument('--batch_size',         default=64,             type=int,     help='')\n",
    "parser.add_argument('--lr',                 default=0.001,           type=float,   help='')\n",
    "parser.add_argument('--hidden_size',        default=800,           type=int,     help='')\n",
    "parser.add_argument('--parameter',          default='',             type=str,     help='')\n",
    "\n",
    "# UCB HYPER-PARAMETERS\n",
    "parser.add_argument('--MC_samples',         default='10',           type=int,     help='Number of Monte Carlo samples')\n",
    "parser.add_argument('--rho',                default='-3',           type=float,   help='Initial rho')\n",
    "parser.add_argument('--sigma1',             default='0.0',          type=float,   help='STD foor the 1st prior pdf in scaled mixture Gaussian')\n",
    "parser.add_argument('--sigma2',             default='6.0',          type=float,   help='STD foor the 2nd prior pdf in scaled mixture Gaussian')\n",
    "parser.add_argument('--pi',                 default='0.25',         type=float,   help='weighting factor for prior')\n",
    "\n",
    "parser.add_argument('--resume',             default='no',           type=str,     help='resume?')\n",
    "parser.add_argument('--sti',                default=1,              type=int,     help='starting task?')\n",
    "\n",
    "parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "\n",
    "args=parser.parse_args()\n",
    "utils.print_arguments(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "16_task_groups_PUGCL\n",
      "Results will be saved in  ../checkpoints_16_tasks/16_task_groups_PUGCL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set seed for stable results\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# Check if Cuda is available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"Using device:\", args.device)\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = utils.make_directories(args)\n",
    "args.checkpoint = checkpoint\n",
    "print()\n",
    "\n",
    "# PUGCL with two tasks:\n",
    "from data import dataloader_16_tasks as dataloader\n",
    "\n",
    "# Import Lifelong Uncertainty-aware Learning approach:\n",
    "#from bayesian_model.lul import Lul\n",
    "from training_method import PUGCL\n",
    "\n",
    "# Import model used:\n",
    "#from bayesian_model.bayesian_network import BayesianNetwork\n",
    "from bayesian_model.bayesian_network import BayesianNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting this session on: \n",
      "2020-05-13 15:21\n",
      "Loading data...\n",
      "Input size = [1, 29] \n",
      "Task info = [(0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (5, 2), (6, 2), (7, 2), (8, 2), (9, 2), (10, 2), (11, 2), (12, 2), (13, 2), (14, 2), (15, 2)]\n",
      "Number of data samples:  500\n",
      "Initializing network...\n",
      "Initialize Lifelong Uncertainty-aware Learning\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonastjomsland/anaconda3/envs/ucb/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Starting this session on: \")\n",
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\n",
    "\n",
    "# Load data:\n",
    "print(\"Loading data...\")\n",
    "data, task_outputs, input_size = dataloader.get(data_path=args.data_path)\n",
    "print(\"Input size =\", input_size, \"\\nTask info =\", task_outputs)\n",
    "print(\"Number of data samples: \", len(data[0]['train']['x']))\n",
    "args.num_tasks = len(task_outputs)\n",
    "args.input_size = input_size\n",
    "args.task_outputs = task_outputs\n",
    "pickle.dump(data, open( \"data/data.p\", \"wb\" ))\n",
    "\n",
    "# Initialize Bayesian network\n",
    "print(\"Initializing network...\")\n",
    "model = BayesianNetwork(args).to(args.device)\n",
    "\n",
    "# Initialize Lul approach\n",
    "print(\"Initialize Lifelong Uncertainty-aware Learning\")\n",
    "approach = PUGCL(model, args=args)\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Check wether resuming:\n",
    "if args.resume == \"yes\":\n",
    "    checkpoint = torch.load(os.path.join(args.checkpoint, 'model_{}.pth.tar'.format(args.sti)))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device=args.device)\n",
    "else:\n",
    "    args.sti = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Task  0 (Vacuum cleaning)\n",
      "****************************************************************************************************\n",
      "Starting training for the tasks in group:  0\n",
      "\r",
      "Batch: 0/450 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/python_arg_parser.cpp:750: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 448/450 | Epoch   1, time=696.9ms/ 13.1ms | Train: loss=1.858 | Valid: loss=1.935 | *\n",
      "Batch: 448/450 | Epoch   2, time=659.0ms/ 13.2ms | Train: loss=1.662 | Valid: loss=1.641 | *\n",
      "Batch: 448/450 | Epoch   3, time=655.1ms/ 12.7ms | Train: loss=1.666 | Valid: loss=1.610 | *\n",
      "Batch: 448/450 | Epoch   4, time=658.3ms/ 13.1ms | Train: loss=1.649 | Valid: loss=1.612 |\n",
      "Batch: 448/450 | Epoch   5, time=656.2ms/ 14.3ms | Train: loss=1.704 | Valid: loss=1.640 |\n",
      "Batch: 448/450 | Epoch   6, time=663.8ms/ 12.8ms | Train: loss=1.624 | Valid: loss=1.577 | *\n",
      "Batch: 448/450 | Epoch   7, time=655.3ms/ 14.9ms | Train: loss=1.649 | Valid: loss=1.626 |\n",
      "Batch: 448/450 | Epoch   8, time=656.5ms/ 12.6ms | Train: loss=1.618 | Valid: loss=1.551 | *\n",
      "Batch: 448/450 | Epoch   9, time=667.3ms/ 15.4ms | Train: loss=1.629 | Valid: loss=1.538 | *\n",
      "Batch: 448/450 | Epoch  10, time=710.4ms/ 15.5ms | Train: loss=1.558 | Valid: loss=1.480 | *\n",
      "Batch: 448/450 | Epoch  11, time=669.0ms/ 13.6ms | Train: loss=1.564 | Valid: loss=1.565 |\n",
      "Batch: 448/450 | Epoch  12, time=658.3ms/ 13.5ms | Train: loss=1.516 | Valid: loss=1.462 | *\n",
      "Batch: 448/450 | Epoch  13, time=658.4ms/ 13.2ms | Train: loss=1.491 | Valid: loss=1.446 | *\n",
      "Batch: 448/450 | Epoch  14, time=655.6ms/ 12.9ms | Train: loss=1.587 | Valid: loss=1.589 |\n",
      "Batch: 448/450 | Epoch  15, time=655.0ms/ 12.7ms | Train: loss=1.562 | Valid: loss=1.573 |\n",
      "Batch: 448/450 | Epoch  16, time=654.5ms/ 13.0ms | Train: loss=1.459 | Valid: loss=1.426 | *\n",
      "Batch: 448/450 | Epoch  17, time=660.1ms/ 17.0ms | Train: loss=1.538 | Valid: loss=1.494 |\n",
      "Batch: 448/450 | Epoch  18, time=672.7ms/ 13.1ms | Train: loss=1.470 | Valid: loss=1.405 | *\n",
      "Batch: 448/450 | Epoch  19, time=657.8ms/ 13.0ms | Train: loss=1.526 | Valid: loss=1.436 |\n",
      "Batch: 448/450 | Epoch  20, time=668.4ms/ 13.8ms | Train: loss=1.498 | Valid: loss=1.434 |\n",
      "Batch: 448/450 | Epoch  21, time=670.2ms/ 13.0ms | Train: loss=1.602 | Valid: loss=1.497 |\n",
      "Batch: 448/450 | Epoch  22, time=665.6ms/ 14.3ms | Train: loss=1.471 | Valid: loss=1.371 | *\n",
      "Batch: 448/450 | Epoch  23, time=677.0ms/ 16.1ms | Train: loss=1.495 | Valid: loss=1.403 |\n",
      "Batch: 448/450 | Epoch  24, time=677.9ms/ 13.2ms | Train: loss=1.432 | Valid: loss=1.409 |\n",
      "Batch: 448/450 | Epoch  25, time=657.1ms/ 12.7ms | Train: loss=1.440 | Valid: loss=1.361 | *\n",
      "Batch: 448/450 | Epoch  26, time=658.8ms/ 12.7ms | Train: loss=1.480 | Valid: loss=1.470 |\n",
      "Batch: 448/450 | Epoch  27, time=668.2ms/ 13.0ms | Train: loss=1.435 | Valid: loss=1.362 |\n",
      "Batch: 448/450 | Epoch  28, time=655.3ms/ 12.7ms | Train: loss=1.422 | Valid: loss=1.381 |\n",
      "Batch: 448/450 | Epoch  29, time=657.7ms/ 12.9ms | Train: loss=1.416 | Valid: loss=1.351 | *\n",
      "Batch: 448/450 | Epoch  30, time=654.2ms/ 12.6ms | Train: loss=1.421 | Valid: loss=1.337 | *\n",
      "Batch: 448/450 | Epoch  31, time=654.7ms/ 12.7ms | Train: loss=1.490 | Valid: loss=1.375 |\n",
      "Batch: 448/450 | Epoch  32, time=655.1ms/ 12.8ms | Train: loss=1.410 | Valid: loss=1.375 |\n",
      "Batch: 448/450 | Epoch  33, time=656.4ms/ 13.1ms | Train: loss=1.442 | Valid: loss=1.406 |\n",
      "Batch: 448/450 | Epoch  34, time=655.7ms/ 12.6ms | Train: loss=1.437 | Valid: loss=1.338 |\n",
      "Batch: 448/450 | Epoch  35, time=657.3ms/ 12.8ms | Train: loss=1.423 | Valid: loss=1.393 |\n",
      "Batch: 448/450 | Epoch  36, time=656.7ms/ 12.7ms | Train: loss=1.466 | Valid: loss=1.398 |\n",
      "Batch: 448/450 | Epoch  37, time=653.5ms/ 12.8ms | Train: loss=1.396 | Valid: loss=1.350 |\n",
      "Batch: 448/450 | Epoch  38, time=656.5ms/ 12.7ms | Train: loss=1.409 | Valid: loss=1.326 | *\n",
      "Batch: 448/450 | Epoch  39, time=653.2ms/ 13.4ms | Train: loss=1.396 | Valid: loss=1.334 |\n",
      "Batch: 448/450 | Epoch  40, time=658.2ms/ 12.9ms | Train: loss=1.404 | Valid: loss=1.331 |\n",
      "Batch: 448/450 | Epoch  41, time=655.8ms/ 12.6ms | Train: loss=1.393 | Valid: loss=1.328 |\n",
      "Batch: 448/450 | Epoch  42, time=655.4ms/ 12.8ms | Train: loss=1.458 | Valid: loss=1.349 |\n",
      "Batch: 448/450 | Epoch  43, time=653.4ms/ 12.7ms | Train: loss=1.430 | Valid: loss=1.327 |\n",
      "Batch: 448/450 | Epoch  44, time=654.0ms/ 12.8ms | Train: loss=1.417 | Valid: loss=1.404 |\n",
      "Batch: 448/450 | Epoch  45, time=654.3ms/ 12.7ms | Train: loss=1.433 | Valid: loss=1.375 |\n",
      "Batch: 448/450 | Epoch  46, time=653.6ms/ 13.0ms | Train: loss=1.398 | Valid: loss=1.341 |\n",
      "Batch: 448/450 | Epoch  47, time=657.6ms/ 12.8ms | Train: loss=1.370 | Valid: loss=1.310 | *\n",
      "Batch: 448/450 | Epoch  48, time=658.1ms/ 12.9ms | Train: loss=1.391 | Valid: loss=1.336 |\n",
      "Batch: 448/450 | Epoch  49, time=659.6ms/ 13.0ms | Train: loss=1.383 | Valid: loss=1.349 |\n",
      "Batch: 448/450 | Epoch  50, time=656.3ms/ 12.9ms | Train: loss=1.366 | Valid: loss=1.311 |\n",
      "Batch: 448/450 | Epoch  51, time=655.9ms/ 13.2ms | Train: loss=1.428 | Valid: loss=1.335 |\n",
      "Batch: 448/450 | Epoch  52, time=654.0ms/ 12.7ms | Train: loss=1.397 | Valid: loss=1.331 |\n",
      "Batch: 448/450 | Epoch  53, time=652.4ms/ 12.5ms | Train: loss=1.398 | Valid: loss=1.346 |\n",
      "Batch: 448/450 | Epoch  54, time=652.8ms/ 12.7ms | Train: loss=1.378 | Valid: loss=1.327 |\n",
      "Batch: 448/450 | Epoch  55, time=654.8ms/ 12.8ms | Train: loss=1.422 | Valid: loss=1.370 |\n",
      "Batch: 448/450 | Epoch  56, time=654.7ms/ 12.6ms | Train: loss=1.424 | Valid: loss=1.338 |\n",
      "Batch: 448/450 | Epoch  57, time=654.7ms/ 12.7ms | Train: loss=1.366 | Valid: loss=1.342 |\n",
      "Batch: 448/450 | Epoch  58, time=652.6ms/ 12.9ms | Train: loss=1.366 | Valid: loss=1.335 |\n",
      "Batch: 448/450 | Epoch  59, time=664.8ms/ 13.9ms | Train: loss=1.414 | Valid: loss=1.419 |\n",
      "Batch: 448/450 | Epoch  60, time=672.2ms/ 13.7ms | Train: loss=1.361 | Valid: loss=1.331 |\n",
      "Batch: 448/450 | Epoch  61, time=658.9ms/ 12.9ms | Train: loss=1.398 | Valid: loss=1.359 |\n",
      "Batch: 448/450 | Epoch  62, time=653.8ms/ 12.7ms | Train: loss=1.481 | Valid: loss=1.439 |\n",
      "Batch: 448/450 | Epoch  63, time=658.0ms/ 12.7ms | Train: loss=1.436 | Valid: loss=1.385 |\n",
      "Batch: 448/450 | Epoch  64, time=653.5ms/ 12.8ms | Train: loss=1.355 | Valid: loss=1.305 | *\n",
      "Batch: 448/450 | Epoch  65, time=652.3ms/ 12.7ms | Train: loss=1.403 | Valid: loss=1.351 |\n",
      "Batch: 448/450 | Epoch  66, time=653.8ms/ 12.8ms | Train: loss=1.424 | Valid: loss=1.440 |\n",
      "Batch: 448/450 | Epoch  67, time=656.7ms/ 12.9ms | Train: loss=1.436 | Valid: loss=1.360 |\n",
      "Batch: 448/450 | Epoch  68, time=659.1ms/ 12.8ms | Train: loss=1.369 | Valid: loss=1.306 |\n",
      "Batch: 448/450 | Epoch  69, time=655.2ms/ 13.0ms | Train: loss=1.391 | Valid: loss=1.346 |\n",
      "Batch: 448/450 | Epoch  70, time=653.0ms/ 12.7ms | Train: loss=1.442 | Valid: loss=1.367 |\n",
      "Batch: 448/450 | Epoch  71, time=653.0ms/ 12.9ms | Train: loss=1.347 | Valid: loss=1.311 |\n",
      "Batch: 448/450 | Epoch  72, time=654.5ms/ 12.7ms | Train: loss=1.356 | Valid: loss=1.315 |\n",
      "Batch: 448/450 | Epoch  73, time=656.5ms/ 13.0ms | Train: loss=1.372 | Valid: loss=1.304 | *\n",
      "Batch: 448/450 | Epoch  74, time=656.2ms/ 12.9ms | Train: loss=1.371 | Valid: loss=1.358 |\n",
      "Batch: 448/450 | Epoch  75, time=659.0ms/ 13.0ms | Train: loss=1.362 | Valid: loss=1.332 |\n",
      "Batch: 448/450 | Epoch  76, time=656.8ms/ 13.1ms | Train: loss=1.357 | Valid: loss=1.326 |\n",
      "Batch: 448/450 | Epoch  77, time=655.9ms/ 13.3ms | Train: loss=1.352 | Valid: loss=1.310 |\n",
      "Batch: 448/450 | Epoch  78, time=666.2ms/ 13.3ms | Train: loss=1.375 | Valid: loss=1.317 |\n",
      "Batch: 448/450 | Epoch  79, time=661.4ms/ 15.5ms | Train: loss=1.403 | Valid: loss=1.364 |\n",
      "Batch: 448/450 | Epoch  80, time=667.5ms/ 13.0ms | Train: loss=1.366 | Valid: loss=1.342 |\n",
      "Batch: 448/450 | Epoch  81, time=658.2ms/ 13.1ms | Train: loss=1.338 | Valid: loss=1.304 | *\n",
      "Batch: 448/450 | Epoch  82, time=668.6ms/ 13.2ms | Train: loss=1.379 | Valid: loss=1.338 |\n",
      "Batch: 448/450 | Epoch  83, time=668.8ms/ 12.8ms | Train: loss=1.378 | Valid: loss=1.312 |\n",
      "Batch: 448/450 | Epoch  84, time=688.9ms/ 12.5ms | Train: loss=1.386 | Valid: loss=1.339 |\n",
      "Batch: 448/450 | Epoch  85, time=686.6ms/ 14.8ms | Train: loss=1.391 | Valid: loss=1.349 |\n",
      "Batch: 448/450 | Epoch  86, time=677.8ms/ 22.4ms | Train: loss=1.403 | Valid: loss=1.400 |\n",
      "Batch: 448/450 | Epoch  87, time=711.6ms/ 13.4ms | Train: loss=1.364 | Valid: loss=1.350 |\n",
      "Batch: 448/450 | Epoch  88, time=687.0ms/ 13.0ms | Train: loss=1.334 | Valid: loss=1.284 | *\n",
      "Batch: 448/450 | Epoch  89, time=675.6ms/ 14.3ms | Train: loss=1.375 | Valid: loss=1.317 |\n",
      "Batch: 448/450 | Epoch  90, time=681.0ms/ 13.7ms | Train: loss=1.332 | Valid: loss=1.293 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 448/450 | Epoch  91, time=670.6ms/ 13.2ms | Train: loss=1.340 | Valid: loss=1.291 |\n",
      "Batch: 448/450 | Epoch  92, time=689.8ms/ 15.3ms | Train: loss=1.390 | Valid: loss=1.342 |\n",
      "Batch: 448/450 | Epoch  93, time=660.0ms/ 15.8ms | Train: loss=1.330 | Valid: loss=1.285 |\n",
      "Batch: 448/450 | Epoch  94, time=680.8ms/ 14.5ms | Train: loss=1.341 | Valid: loss=1.288 |\n",
      "Batch: 448/450 | Epoch  95, time=678.1ms/ 13.9ms | Train: loss=1.443 | Valid: loss=1.365 |\n",
      "Batch: 448/450 | Epoch  96, time=684.6ms/ 15.7ms | Train: loss=1.411 | Valid: loss=1.342 |\n",
      "Batch: 448/450 | Epoch  97, time=681.3ms/ 14.8ms | Train: loss=1.482 | Valid: loss=1.506 |\n",
      "Batch: 448/450 | Epoch  98, time=662.1ms/ 12.7ms | Train: loss=1.323 | Valid: loss=1.278 | *\n",
      "Batch: 448/450 | Epoch  99, time=650.8ms/ 12.7ms | Train: loss=1.334 | Valid: loss=1.309 |\n",
      "Batch: 448/450 | Epoch 100, time=678.4ms/ 13.6ms | Train: loss=1.336 | Valid: loss=1.298 |\n",
      "____________________________________________________________________________________________________\n",
      "Test on task  0 - Vacuum cleaning: loss=1.430\n",
      "Saving at ../checkpoints_16_tasks/16_task_groups_PUGCL\n",
      "****************************************************************************************************\n",
      "Task  1 (Mopping the floor)\n",
      "****************************************************************************************************\n",
      "Starting training for the tasks in group:  1\n",
      "Batch: 448/450 | Epoch   1, time=653.2ms/ 12.9ms | Train: loss=4.170 | Valid: loss=3.902 | *\n",
      "Batch: 448/450 | Epoch   2, time=658.4ms/ 13.7ms | Train: loss=4.161 | Valid: loss=3.888 | *\n",
      "Batch: 448/450 | Epoch   3, time=665.3ms/ 13.6ms | Train: loss=4.151 | Valid: loss=3.874 | *\n",
      "Batch: 448/450 | Epoch   4, time=660.6ms/ 13.1ms | Train: loss=4.131 | Valid: loss=3.852 | *\n",
      "Batch: 448/450 | Epoch   5, time=661.4ms/ 13.2ms | Train: loss=4.075 | Valid: loss=3.798 | *\n",
      "Batch: 448/450 | Epoch   6, time=654.4ms/ 13.1ms | Train: loss=4.025 | Valid: loss=3.750 | *\n",
      "Batch: 448/450 | Epoch   7, time=652.6ms/ 12.8ms | Train: loss=4.022 | Valid: loss=3.743 | *\n",
      "Batch: 448/450 | Epoch   8, time=649.6ms/ 12.7ms | Train: loss=3.960 | Valid: loss=3.685 | *\n",
      "Batch: 448/450 | Epoch   9, time=655.5ms/ 13.2ms | Train: loss=3.935 | Valid: loss=3.659 | *\n",
      "Batch: 448/450 | Epoch  10, time=661.1ms/ 13.3ms | Train: loss=3.849 | Valid: loss=3.578 | *\n",
      "Batch: 448/450 | Epoch  11, time=659.9ms/ 13.7ms | Train: loss=3.833 | Valid: loss=3.562 | *\n",
      "Batch: 448/450 | Epoch  12, time=893.2ms/ 20.9ms | Train: loss=3.775 | Valid: loss=3.506 | *\n",
      "Batch: 448/450 | Epoch  13, time=759.2ms/ 20.1ms | Train: loss=3.712 | Valid: loss=3.447 | *\n",
      "Batch: 448/450 | Epoch  14, time=723.2ms/ 13.3ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  15, time=668.4ms/ 13.2ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  16, time=725.3ms/ 14.4ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  17, time=701.7ms/ 13.5ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  18, time=687.0ms/ 13.1ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  19, time=678.8ms/ 13.3ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  20, time=674.8ms/ 13.2ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  21, time=690.6ms/ 13.9ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  22, time=733.5ms/ 16.3ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  23, time=734.1ms/ 17.5ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  24, time=727.3ms/ 19.5ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  25, time=736.5ms/ 18.1ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  26, time=743.8ms/ 17.6ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  27, time=737.9ms/ 18.2ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  28, time=744.0ms/ 17.7ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  29, time=708.3ms/ 14.7ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  30, time=678.8ms/ 14.9ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  31, time=680.1ms/ 13.9ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  32, time=680.4ms/ 13.2ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  33, time=694.6ms/ 14.7ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  34, time=685.6ms/ 13.6ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  35, time=674.4ms/ 13.0ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  36, time=648.9ms/ 12.9ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  37, time=657.1ms/ 14.2ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  38, time=689.3ms/ 13.0ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  39, time=670.4ms/ 13.6ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  40, time=696.6ms/ 13.6ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  41, time=684.0ms/ 14.2ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  42, time=663.7ms/ 18.2ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  43, time=663.1ms/ 13.4ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  44, time=660.1ms/ 13.1ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  45, time=659.3ms/ 13.4ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  46, time=655.5ms/ 13.4ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  47, time=666.3ms/ 16.4ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  48, time=672.6ms/ 14.4ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  49, time=689.0ms/ 15.4ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  50, time=723.7ms/ 16.4ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  51, time=693.9ms/ 16.0ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  52, time=727.7ms/ 13.8ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  53, time=697.8ms/ 19.5ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  54, time=683.9ms/ 14.6ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  55, time=676.1ms/ 13.5ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 448/450 | Epoch  56, time=686.3ms/ 13.1ms | Train: loss=3.733 | Valid: loss=3.463 |\n",
      "Batch: 320/450 "
     ]
    }
   ],
   "source": [
    "# Iterate over the two tasks:\n",
    "loss = np.zeros((len(task_outputs), len(task_outputs)), dtype=np.float32)\n",
    "for task, n_class in task_outputs[args.sti:]:\n",
    "    print('*'*100)\n",
    "    print('Task {:2d} ({:s})'.format(task, data[task]['name']))\n",
    "    print('*'*100)\n",
    "\n",
    "    # Get data:\n",
    "    xtrain = data[task]['train']['x'][:,1:].type(torch.float32).to(args.device)\n",
    "    ytrain = data[task]['train']['y'].type(torch.float32).to(args.device)\n",
    "\n",
    "    # Start training\n",
    "    print(\"Starting training for the tasks in group: \", task)\n",
    "    approach.train(task, xtrain, ytrain)\n",
    "    print('_'*100)\n",
    "\n",
    "    # Test for this task group:\n",
    "    for u in range(task+1):\n",
    "        xtest = data[u]['test']['x'][:,1:].type(torch.float32).to(args.device)\n",
    "        ytest = data[u]['test']['y'].type(torch.float32).to(args.device)\n",
    "        test_loss = approach.eval(u, xtest, ytest, debug=True)\n",
    "        print(\"Test on task {:2d} - {:15s}: loss={:.3f}\".format(u, data[u]['name'], test_loss))\n",
    "        loss[task, u] = test_loss\n",
    "\n",
    "    # Save\n",
    "    print(\"Saving at \" + args.checkpoint)\n",
    "    np.savetxt(os.path.join(args.checkpoint, '{}_{}_{}.txt'.format(args.experiment, \"Lul\", args.seed)), loss, '%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
