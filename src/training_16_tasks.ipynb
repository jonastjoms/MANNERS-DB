{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,argparse,time\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "tstart=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Arguments =\n",
      "\tseed: 0\n",
      "\tdevice: cpu\n",
      "\texperiment: 16_task_groups\n",
      "\tapproach: PUGCL\n",
      "\tdata_path: data/data.csv\n",
      "\toutput: \n",
      "\tcheckpoint_dir: ../checkpoints_16_tasks\n",
      "\tn_epochs: 200\n",
      "\tbatch_size: 64\n",
      "\tlr: 0.03\n",
      "\thidden_size: 800\n",
      "\tparameter: \n",
      "\tMC_samples: 10\n",
      "\trho: -3.0\n",
      "\tsigma1: 0.0\n",
      "\tsigma2: 6.0\n",
      "\tpi: 0.25\n",
      "\tresume: no\n",
      "\tsti: 1\n",
      "\tfff: /Users/jonastjomsland/Library/Jupyter/runtime/kernel-003dd4ae-5b1e-4b75-9dcd-182fa5aa8112.json\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Arguments\n",
    "parser=argparse.ArgumentParser(description='xxx')\n",
    "parser.add_argument('--seed',               default=0,              type=int,     help='(default=%(default)d)')\n",
    "parser.add_argument('--device',             default='cpu',          type=str,     help='gpu id')\n",
    "parser.add_argument('--experiment',         default='16_task_groups',       type =str,    help='Mnist or dissertation')\n",
    "parser.add_argument('--approach',           default='PUGCL',          type =str,    help='Method, always Lifelong Uncertainty-aware learning')\n",
    "parser.add_argument('--data_path',          default='data/data.csv',     type=str,     help='gpu id')\n",
    "\n",
    "# Training parameters\n",
    "parser.add_argument('--output',             default='',             type=str,     help='')\n",
    "parser.add_argument('--checkpoint_dir',     default='../checkpoints_16_tasks',    type=str,   help='')\n",
    "parser.add_argument('--n_epochs',           default= 200,              type=int,     help='')\n",
    "parser.add_argument('--batch_size',         default=64,             type=int,     help='')\n",
    "parser.add_argument('--lr',                 default=0.03,           type=float,   help='')\n",
    "parser.add_argument('--hidden_size',        default=800,           type=int,     help='')\n",
    "parser.add_argument('--parameter',          default='',             type=str,     help='')\n",
    "\n",
    "# UCB HYPER-PARAMETERS\n",
    "parser.add_argument('--MC_samples',         default='10',           type=int,     help='Number of Monte Carlo samples')\n",
    "parser.add_argument('--rho',                default='-3',           type=float,   help='Initial rho')\n",
    "parser.add_argument('--sigma1',             default='0.0',          type=float,   help='STD foor the 1st prior pdf in scaled mixture Gaussian')\n",
    "parser.add_argument('--sigma2',             default='6.0',          type=float,   help='STD foor the 2nd prior pdf in scaled mixture Gaussian')\n",
    "parser.add_argument('--pi',                 default='0.25',         type=float,   help='weighting factor for prior')\n",
    "\n",
    "parser.add_argument('--resume',             default='no',           type=str,     help='resume?')\n",
    "parser.add_argument('--sti',                default=1,              type=int,     help='starting task?')\n",
    "\n",
    "parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "\n",
    "args=parser.parse_args()\n",
    "utils.print_arguments(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "16_task_groups_PUGCL\n",
      "Results will be saved in  ../checkpoints_16_tasks/16_task_groups_PUGCL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set seed for stable results\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# Check if Cuda is available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"Using device:\", args.device)\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = utils.make_directories(args)\n",
    "args.checkpoint = checkpoint\n",
    "print()\n",
    "\n",
    "# PUGCL with two tasks:\n",
    "from data import dataloader_16_tasks as dataloader\n",
    "\n",
    "# Import Lifelong Uncertainty-aware Learning approach:\n",
    "#from bayesian_model.lul import Lul\n",
    "from training_method import PUGCL\n",
    "\n",
    "# Import model used:\n",
    "#from bayesian_model.bayesian_network import BayesianNetwork\n",
    "from bayesian_model.bayesian_network import BayesianNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting this session on: \n",
      "2020-05-14 21:22\n",
      "Loading data...\n",
      "Input size = [1, 29] \n",
      "Task info = [(0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (5, 2), (6, 2), (7, 2), (8, 2), (9, 2), (10, 2), (11, 2), (12, 2), (13, 2), (14, 2), (15, 2)]\n",
      "Number of data samples:  500\n",
      "Initializing network...\n",
      "Initialize Lifelong Uncertainty-aware Learning\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonastjomsland/anaconda3/envs/ucb/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Starting this session on: \")\n",
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\n",
    "\n",
    "# Load data:\n",
    "print(\"Loading data...\")\n",
    "data, task_outputs, input_size = dataloader.get(data_path=args.data_path)\n",
    "print(\"Input size =\", input_size, \"\\nTask info =\", task_outputs)\n",
    "print(\"Number of data samples: \", len(data[0]['train']['x']))\n",
    "args.num_tasks = len(task_outputs)\n",
    "args.input_size = input_size\n",
    "args.task_outputs = task_outputs\n",
    "pickle.dump(data, open( \"data/data.p\", \"wb\" ))\n",
    "\n",
    "# Initialize Bayesian network\n",
    "print(\"Initializing network...\")\n",
    "model = BayesianNetwork(args).to(args.device)\n",
    "\n",
    "# Initialize Lul approach\n",
    "print(\"Initialize Lifelong Uncertainty-aware Learning\")\n",
    "approach = PUGCL(model, args=args)\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Check wether resuming:\n",
    "if args.resume == \"yes\":\n",
    "    checkpoint = torch.load(os.path.join(args.checkpoint, 'model_{}.pth.tar'.format(args.sti)))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device=args.device)\n",
    "else:\n",
    "    args.sti = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Task  0 (Vacuum cleaning)\n",
      "****************************************************************************************************\n",
      "Starting training for the tasks in group:  0\n",
      "\r",
      "Batch: 0/500 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../torch/csrc/utils/python_arg_parser.cpp:750: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 448/500 | Epoch   1, time=621.6ms/ 11.6ms | Training loss: 1.699 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch   2, time=590.5ms/ 11.7ms | Training loss: 1.627 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch   3, time=683.0ms/ 36.5ms | Training loss: 1.594 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch   4, time=787.5ms/ 18.5ms | Training loss: 1.557 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch   5, time=630.3ms/ 13.0ms | Training loss: 1.514 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch   6, time=615.3ms/ 11.8ms | Training loss: 1.608 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch   7, time=623.5ms/ 12.5ms | Training loss: 1.584 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch   8, time=605.6ms/ 13.2ms | Training loss: 1.592 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch   9, time=603.3ms/ 12.3ms | Training loss: 1.488 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  10, time=628.8ms/ 12.1ms | Training loss: 1.566 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  11, time=589.6ms/ 11.8ms | Training loss: 1.471 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  12, time=585.1ms/ 11.6ms | Training loss: 1.424 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  13, time=579.0ms/ 11.7ms | Training loss: 1.459 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  14, time=580.1ms/ 11.7ms | Training loss: 1.400 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  15, time=580.6ms/ 11.8ms | Training loss: 1.399 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  16, time=582.5ms/ 11.6ms | Training loss: 1.386 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  17, time=580.6ms/ 12.1ms | Training loss: 1.387 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  18, time=579.9ms/ 11.6ms | Training loss: 1.381 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  19, time=581.8ms/ 11.6ms | Training loss: 1.458 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  20, time=579.5ms/ 11.6ms | Training loss: 1.391 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  21, time=583.3ms/ 12.0ms | Training loss: 1.436 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  22, time=581.7ms/ 11.6ms | Training loss: 1.367 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  23, time=585.2ms/ 11.7ms | Training loss: 1.401 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  24, time=582.0ms/ 11.5ms | Training loss: 1.351 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  25, time=580.9ms/ 11.7ms | Training loss: 1.394 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  26, time=585.5ms/ 11.7ms | Training loss: 1.396 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  27, time=581.3ms/ 12.0ms | Training loss: 1.410 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  28, time=581.1ms/ 12.0ms | Training loss: 1.363 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  29, time=581.0ms/ 11.8ms | Training loss: 1.337 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  30, time=580.6ms/ 11.6ms | Training loss: 1.341 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  31, time=581.9ms/ 11.6ms | Training loss: 1.339 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  32, time=581.8ms/ 11.6ms | Training loss: 1.409 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  33, time=580.5ms/ 12.4ms | Training loss: 1.345 | Learning rate: 0.030 |\n",
      "Batch: 448/500 | Epoch  34, time=581.9ms/ 11.6ms | Training loss: 1.393 | Learning rate: 0.030 | lr=1.0e-02\n",
      "Batch: 448/500 | Epoch  35, time=583.0ms/ 11.5ms | Training loss: 1.329 | Learning rate: 0.010 | *\n",
      "Batch: 448/500 | Epoch  36, time=582.9ms/ 11.8ms | Training loss: 1.331 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  37, time=582.3ms/ 11.9ms | Training loss: 1.406 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  38, time=582.9ms/ 11.6ms | Training loss: 1.328 | Learning rate: 0.010 | *\n",
      "Batch: 448/500 | Epoch  39, time=582.8ms/ 11.7ms | Training loss: 1.376 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  40, time=581.9ms/ 11.6ms | Training loss: 1.337 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  41, time=583.9ms/ 11.4ms | Training loss: 1.401 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  42, time=584.5ms/ 12.7ms | Training loss: 1.327 | Learning rate: 0.010 | *\n",
      "Batch: 448/500 | Epoch  43, time=590.7ms/ 11.5ms | Training loss: 1.315 | Learning rate: 0.010 | *\n",
      "Batch: 448/500 | Epoch  44, time=584.1ms/ 11.7ms | Training loss: 1.380 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  45, time=585.7ms/ 11.4ms | Training loss: 1.336 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  46, time=584.4ms/ 13.9ms | Training loss: 1.397 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  47, time=588.8ms/ 12.1ms | Training loss: 1.307 | Learning rate: 0.010 | *\n",
      "Batch: 448/500 | Epoch  48, time=610.2ms/ 11.9ms | Training loss: 1.337 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  49, time=586.5ms/ 11.8ms | Training loss: 1.331 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  50, time=588.6ms/ 11.8ms | Training loss: 1.392 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  51, time=584.8ms/ 11.8ms | Training loss: 1.335 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  52, time=584.9ms/ 11.7ms | Training loss: 1.305 | Learning rate: 0.010 | *\n",
      "Batch: 448/500 | Epoch  53, time=584.6ms/ 11.6ms | Training loss: 1.331 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  54, time=590.4ms/ 11.6ms | Training loss: 1.388 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  55, time=587.5ms/ 14.1ms | Training loss: 1.338 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  56, time=585.7ms/ 12.0ms | Training loss: 1.333 | Learning rate: 0.010 |\n",
      "Batch: 448/500 | Epoch  57, time=591.1ms/ 11.6ms | Training loss: 1.325 | Learning rate: 0.010 | lr=3.3e-03\n",
      "Batch: 448/500 | Epoch  58, time=589.3ms/ 11.6ms | Training loss: 1.322 | Learning rate: 0.003 |\n",
      "Batch: 448/500 | Epoch  59, time=586.6ms/ 11.9ms | Training loss: 1.384 | Learning rate: 0.003 |\n",
      "Batch: 448/500 | Epoch  60, time=591.6ms/ 11.6ms | Training loss: 1.298 | Learning rate: 0.003 | *\n",
      "Batch: 448/500 | Epoch  61, time=586.7ms/ 11.4ms | Training loss: 1.296 | Learning rate: 0.003 | *\n",
      "Batch: 448/500 | Epoch  62, time=583.9ms/ 11.4ms | Training loss: 1.358 | Learning rate: 0.003 |\n",
      "Batch: 448/500 | Epoch  63, time=584.4ms/ 12.2ms | Training loss: 1.298 | Learning rate: 0.003 |\n",
      "Batch: 448/500 | Epoch  64, time=585.5ms/ 11.7ms | Training loss: 1.299 | Learning rate: 0.003 |\n",
      "Batch: 448/500 | Epoch  65, time=585.0ms/ 11.7ms | Training loss: 1.317 | Learning rate: 0.003 |\n",
      "Batch: 448/500 | Epoch  66, time=607.4ms/ 14.0ms | Training loss: 1.322 | Learning rate: 0.003 | lr=1.1e-03\n",
      "Batch: 448/500 | Epoch  67, time=609.1ms/ 11.9ms | Training loss: 1.308 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  68, time=585.2ms/ 11.5ms | Training loss: 1.293 | Learning rate: 0.001 | *\n",
      "Batch: 448/500 | Epoch  69, time=585.1ms/ 11.8ms | Training loss: 1.312 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  70, time=586.5ms/ 11.8ms | Training loss: 1.288 | Learning rate: 0.001 | *\n",
      "Batch: 448/500 | Epoch  71, time=583.9ms/ 11.5ms | Training loss: 1.350 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  72, time=586.8ms/ 11.6ms | Training loss: 1.287 | Learning rate: 0.001 | *\n",
      "Batch: 448/500 | Epoch  73, time=584.3ms/ 11.7ms | Training loss: 1.348 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  74, time=585.8ms/ 12.3ms | Training loss: 1.298 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  75, time=582.1ms/ 11.6ms | Training loss: 1.298 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  76, time=569.7ms/ 11.6ms | Training loss: 1.324 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  77, time=572.0ms/ 11.7ms | Training loss: 1.293 | Learning rate: 0.001 | lr=3.7e-04\n",
      "Batch: 448/500 | Epoch  78, time=570.4ms/ 11.6ms | Training loss: 1.342 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  79, time=569.4ms/ 11.8ms | Training loss: 1.290 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  80, time=571.1ms/ 11.6ms | Training loss: 1.291 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  81, time=568.7ms/ 11.5ms | Training loss: 1.280 | Learning rate: 0.000 | *\n",
      "Batch: 448/500 | Epoch  82, time=569.4ms/ 11.7ms | Training loss: 1.288 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  83, time=571.5ms/ 11.5ms | Training loss: 1.331 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  84, time=569.9ms/ 11.8ms | Training loss: 1.391 | Learning rate: 0.000 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 448/500 | Epoch  85, time=569.4ms/ 11.6ms | Training loss: 1.333 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  86, time=569.7ms/ 12.7ms | Training loss: 1.279 | Learning rate: 0.000 | *\n",
      "Batch: 448/500 | Epoch  87, time=570.4ms/ 12.2ms | Training loss: 1.277 | Learning rate: 0.000 | *\n",
      "Batch: 448/500 | Epoch  88, time=565.5ms/ 11.5ms | Training loss: 1.288 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  89, time=565.6ms/ 11.8ms | Training loss: 1.314 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  90, time=566.1ms/ 11.6ms | Training loss: 1.299 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  91, time=565.3ms/ 11.6ms | Training loss: 1.276 | Learning rate: 0.000 | *\n",
      "Batch: 448/500 | Epoch  92, time=565.3ms/ 11.7ms | Training loss: 1.325 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  93, time=565.9ms/ 11.5ms | Training loss: 1.290 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  94, time=565.7ms/ 11.5ms | Training loss: 1.306 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  95, time=565.9ms/ 11.6ms | Training loss: 1.275 | Learning rate: 0.000 | *\n",
      "Batch: 448/500 | Epoch  96, time=565.4ms/ 12.5ms | Training loss: 1.319 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  97, time=569.3ms/ 11.8ms | Training loss: 1.323 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  98, time=567.9ms/ 11.8ms | Training loss: 1.295 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  99, time=569.9ms/ 14.2ms | Training loss: 1.280 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 100, time=578.5ms/ 23.8ms | Training loss: 1.275 | Learning rate: 0.000 | *\n",
      "Batch: 448/500 | Epoch 101, time=594.8ms/ 11.6ms | Training loss: 1.354 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 102, time=583.5ms/ 12.0ms | Training loss: 1.284 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 103, time=584.4ms/ 11.7ms | Training loss: 1.282 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 104, time=585.1ms/ 11.7ms | Training loss: 1.278 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 105, time=583.9ms/ 11.6ms | Training loss: 1.282 | Learning rate: 0.000 | lr=1.2e-04\n",
      "Batch: 448/500 | Epoch 106, time=582.7ms/ 11.7ms | Training loss: 1.344 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 107, time=579.0ms/ 11.5ms | Training loss: 1.286 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 108, time=572.9ms/ 11.5ms | Training loss: 1.273 | Learning rate: 0.000 | *\n",
      "Batch: 448/500 | Epoch 109, time=572.1ms/ 11.7ms | Training loss: 1.271 | Learning rate: 0.000 | *\n",
      "Batch: 448/500 | Epoch 110, time=571.1ms/ 11.6ms | Training loss: 1.277 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 111, time=594.9ms/ 11.7ms | Training loss: 1.277 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 112, time=578.2ms/ 11.5ms | Training loss: 1.310 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 113, time=570.8ms/ 11.8ms | Training loss: 1.327 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 114, time=571.2ms/ 11.7ms | Training loss: 1.311 | Learning rate: 0.000 | lr=4.1e-05\n",
      "Batch: 448/500 | Epoch 115, time=570.2ms/ 11.8ms | Training loss: 1.273 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 116, time=572.4ms/ 11.7ms | Training loss: 1.271 | Learning rate: 0.000 | *\n",
      "Batch: 448/500 | Epoch 117, time=573.9ms/ 11.6ms | Training loss: 1.277 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 118, time=570.8ms/ 11.8ms | Training loss: 1.361 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 119, time=574.0ms/ 11.6ms | Training loss: 1.282 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 120, time=570.8ms/ 11.5ms | Training loss: 1.270 | Learning rate: 0.000 | *\n",
      "Batch: 448/500 | Epoch 121, time=575.1ms/ 11.5ms | Training loss: 1.276 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 122, time=573.0ms/ 12.1ms | Training loss: 1.282 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 123, time=575.7ms/ 11.5ms | Training loss: 1.264 | Learning rate: 0.000 | *\n",
      "Batch: 448/500 | Epoch 124, time=577.0ms/ 11.7ms | Training loss: 1.281 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 125, time=574.4ms/ 12.4ms | Training loss: 1.276 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 126, time=571.9ms/ 11.6ms | Training loss: 1.275 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 127, time=571.3ms/ 11.7ms | Training loss: 1.277 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 128, time=573.1ms/ 11.4ms | Training loss: 1.267 | Learning rate: 0.000 | lr=1.4e-05\n",
      "Batch: 448/500 | Epoch 129, time=571.3ms/ 11.7ms | Training loss: 1.293 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 130, time=572.4ms/ 11.4ms | Training loss: 1.277 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 131, time=571.6ms/ 11.6ms | Training loss: 1.273 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 132, time=573.8ms/ 11.6ms | Training loss: 1.295 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 133, time=572.5ms/ 11.7ms | Training loss: 1.282 | Learning rate: 0.000 | lr=4.6e-06\n",
      "Batch: 448/500 | Epoch 134, time=572.5ms/ 11.5ms | Training loss: 1.343 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 135, time=571.3ms/ 11.7ms | Training loss: 1.286 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 136, time=572.0ms/ 11.9ms | Training loss: 1.278 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 137, time=573.3ms/ 13.2ms | Training loss: 1.260 | Learning rate: 0.000 | *\n",
      "Batch: 448/500 | Epoch 138, time=578.0ms/ 11.7ms | Training loss: 1.312 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 139, time=571.8ms/ 11.6ms | Training loss: 1.289 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 140, time=572.5ms/ 11.5ms | Training loss: 1.291 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 141, time=573.7ms/ 11.6ms | Training loss: 1.269 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 142, time=572.7ms/ 11.7ms | Training loss: 1.269 | Learning rate: 0.000 | lr=1.5e-06\n",
      "Batch: 448/500 | Epoch 143, time=572.2ms/ 11.6ms | Training loss: 1.280 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 144, time=571.9ms/ 11.6ms | Training loss: 1.293 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 145, time=574.5ms/ 11.7ms | Training loss: 1.270 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 146, time=572.5ms/ 11.8ms | Training loss: 1.264 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 147, time=573.8ms/ 11.9ms | Training loss: 1.280 | Learning rate: 0.000 | lr=5.1e-07\n",
      "____________________________________________________________________________________________________\n",
      "Test on task  0 - Vacuum cleaning: loss=1.403\n",
      "Saving at ../checkpoints_16_tasks/16_task_groups_PUGCL\n",
      "****************************************************************************************************\n",
      "Task  1 (Mopping the floor)\n",
      "****************************************************************************************************\n",
      "Starting training for the tasks in group:  1\n",
      "Batch: 448/500 | Epoch   1, time=577.5ms/ 11.6ms | Training loss: 2.419 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   2, time=575.5ms/ 11.6ms | Training loss: 1.783 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   3, time=572.2ms/ 11.7ms | Training loss: 1.700 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   4, time=573.1ms/ 11.7ms | Training loss: 1.520 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   5, time=574.5ms/ 11.6ms | Training loss: 1.885 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch   6, time=575.3ms/ 11.9ms | Training loss: 1.443 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   7, time=573.8ms/ 11.5ms | Training loss: 1.455 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch   8, time=572.7ms/ 11.6ms | Training loss: 1.566 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch   9, time=574.2ms/ 11.8ms | Training loss: 1.600 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  10, time=573.5ms/ 11.8ms | Training loss: 1.555 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  11, time=573.3ms/ 12.4ms | Training loss: 1.501 | Learning rate: 0.050 | lr=1.7e-02\n",
      "Batch: 448/500 | Epoch  12, time=571.5ms/ 11.7ms | Training loss: 1.501 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  13, time=573.3ms/ 11.5ms | Training loss: 1.501 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  14, time=573.1ms/ 11.4ms | Training loss: 1.501 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  15, time=573.3ms/ 11.5ms | Training loss: 1.501 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  16, time=572.7ms/ 11.7ms | Training loss: 1.501 | Learning rate: 0.017 | lr=5.6e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 448/500 | Epoch  17, time=582.6ms/ 11.7ms | Training loss: 1.501 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  18, time=577.7ms/ 11.7ms | Training loss: 1.501 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  19, time=576.8ms/ 11.5ms | Training loss: 1.501 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  20, time=575.7ms/ 11.5ms | Training loss: 1.501 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  21, time=573.1ms/ 11.6ms | Training loss: 1.501 | Learning rate: 0.006 | lr=1.9e-03\n",
      "Batch: 448/500 | Epoch  22, time=574.1ms/ 12.0ms | Training loss: 1.501 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  23, time=575.8ms/ 11.5ms | Training loss: 1.501 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  24, time=574.0ms/ 12.0ms | Training loss: 1.501 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  25, time=571.9ms/ 11.9ms | Training loss: 1.501 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  26, time=575.3ms/ 11.6ms | Training loss: 1.501 | Learning rate: 0.002 | lr=6.2e-04\n",
      "Batch: 448/500 | Epoch  27, time=574.9ms/ 11.6ms | Training loss: 1.501 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  28, time=577.5ms/ 12.0ms | Training loss: 1.501 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  29, time=604.0ms/ 11.7ms | Training loss: 1.501 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  30, time=574.8ms/ 11.4ms | Training loss: 1.501 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  31, time=575.6ms/ 11.6ms | Training loss: 1.501 | Learning rate: 0.001 | lr=2.1e-04\n",
      "Batch: 448/500 | Epoch  32, time=574.4ms/ 12.3ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  33, time=573.0ms/ 11.6ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  34, time=573.3ms/ 11.7ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  35, time=573.0ms/ 11.6ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  36, time=575.9ms/ 12.3ms | Training loss: 1.501 | Learning rate: 0.000 | lr=6.9e-05\n",
      "Batch: 448/500 | Epoch  37, time=574.1ms/ 12.2ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  38, time=575.4ms/ 11.9ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  39, time=575.8ms/ 11.5ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  40, time=573.3ms/ 11.7ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  41, time=578.0ms/ 12.4ms | Training loss: 1.501 | Learning rate: 0.000 | lr=2.3e-05\n",
      "Batch: 448/500 | Epoch  42, time=576.0ms/ 11.5ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  43, time=574.8ms/ 11.7ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  44, time=573.3ms/ 11.7ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  45, time=573.8ms/ 11.8ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  46, time=573.3ms/ 11.6ms | Training loss: 1.501 | Learning rate: 0.000 | lr=7.6e-06\n",
      "Batch: 448/500 | Epoch  47, time=573.2ms/ 11.7ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  48, time=573.5ms/ 11.6ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  49, time=573.4ms/ 11.6ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  50, time=573.4ms/ 11.5ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  51, time=575.8ms/ 11.9ms | Training loss: 1.501 | Learning rate: 0.000 | lr=2.5e-06\n",
      "Batch: 448/500 | Epoch  52, time=605.6ms/ 22.5ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  53, time=588.5ms/ 11.5ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  54, time=580.5ms/ 11.7ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  55, time=583.2ms/ 11.7ms | Training loss: 1.501 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  56, time=67422.9ms/ 16.4ms | Training loss: 1.501 | Learning rate: 0.000 | lr=8.5e-07\n",
      "____________________________________________________________________________________________________\n",
      "Test on task  0 - Vacuum cleaning: loss=1.440\n",
      "Test on task  1 - Mopping the floor: loss=1.495\n",
      "Saving at ../checkpoints_16_tasks/16_task_groups_PUGCL\n",
      "****************************************************************************************************\n",
      "Task  2 (Carry warm food)\n",
      "****************************************************************************************************\n",
      "Starting training for the tasks in group:  2\n",
      "Batch: 448/500 | Epoch   1, time=723.0ms/ 11.5ms | Training loss: 1.752 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   2, time=1458.2ms/114.9ms | Training loss: 1.550 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   3, time=37870.9ms/ 23.8ms | Training loss: 1.470 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   4, time=656.8ms/ 18.8ms | Training loss: 1.447 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   5, time=610.0ms/ 13.1ms | Training loss: 1.464 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch   6, time=603.2ms/ 12.1ms | Training loss: 1.426 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   7, time=650.3ms/ 12.4ms | Training loss: 1.416 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   8, time=601.3ms/ 12.3ms | Training loss: 1.476 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch   9, time=600.0ms/ 21.3ms | Training loss: 1.383 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  10, time=592.0ms/ 11.7ms | Training loss: 1.439 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  11, time=587.1ms/ 11.5ms | Training loss: 1.403 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  12, time=587.3ms/ 11.7ms | Training loss: 1.358 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  13, time=591.1ms/ 12.2ms | Training loss: 1.369 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  14, time=586.4ms/ 11.7ms | Training loss: 1.378 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  15, time=595.4ms/ 11.7ms | Training loss: 1.366 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  16, time=586.6ms/ 11.7ms | Training loss: 1.351 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  17, time=595.5ms/ 11.7ms | Training loss: 1.418 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  18, time=607.5ms/ 15.1ms | Training loss: 1.426 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  19, time=605.0ms/ 13.6ms | Training loss: 1.335 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  20, time=599.1ms/ 13.6ms | Training loss: 1.474 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  21, time=643.2ms/ 11.9ms | Training loss: 1.328 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  22, time=586.9ms/ 11.6ms | Training loss: 1.369 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  23, time=603.8ms/ 12.4ms | Training loss: 1.347 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  24, time=593.5ms/ 12.4ms | Training loss: 1.348 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  25, time=600.7ms/ 12.1ms | Training loss: 1.369 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  26, time=604.8ms/ 12.7ms | Training loss: 1.370 | Learning rate: 0.050 | lr=1.7e-02\n",
      "Batch: 448/500 | Epoch  27, time=597.1ms/ 12.1ms | Training loss: 1.370 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  28, time=597.8ms/ 12.8ms | Training loss: 1.370 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  29, time=594.7ms/ 12.2ms | Training loss: 1.370 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  30, time=606.6ms/ 13.1ms | Training loss: 1.370 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  31, time=594.0ms/ 11.6ms | Training loss: 1.370 | Learning rate: 0.017 | lr=5.6e-03\n",
      "Batch: 448/500 | Epoch  32, time=598.3ms/ 13.0ms | Training loss: 1.370 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  33, time=600.5ms/ 12.3ms | Training loss: 1.370 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  34, time=598.3ms/ 11.7ms | Training loss: 1.370 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  35, time=596.1ms/ 12.1ms | Training loss: 1.370 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  36, time=596.1ms/ 12.2ms | Training loss: 1.370 | Learning rate: 0.006 | lr=1.9e-03\n",
      "Batch: 448/500 | Epoch  37, time=603.0ms/ 11.9ms | Training loss: 1.370 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  38, time=593.6ms/ 12.1ms | Training loss: 1.370 | Learning rate: 0.002 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 448/500 | Epoch  39, time=597.6ms/ 12.4ms | Training loss: 1.370 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  40, time=597.6ms/ 11.9ms | Training loss: 1.370 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  41, time=596.8ms/ 12.2ms | Training loss: 1.370 | Learning rate: 0.002 | lr=6.2e-04\n",
      "Batch: 448/500 | Epoch  42, time=597.6ms/ 12.7ms | Training loss: 1.370 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  43, time=598.6ms/ 12.1ms | Training loss: 1.370 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  44, time=599.9ms/ 12.2ms | Training loss: 1.370 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  45, time=603.2ms/ 13.1ms | Training loss: 1.370 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  46, time=594.6ms/ 12.1ms | Training loss: 1.370 | Learning rate: 0.001 | lr=2.1e-04\n",
      "Batch: 448/500 | Epoch  47, time=596.1ms/ 12.1ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  48, time=595.4ms/ 12.1ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  49, time=604.1ms/ 12.1ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  50, time=581.6ms/ 12.2ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  51, time=590.3ms/ 12.1ms | Training loss: 1.370 | Learning rate: 0.000 | lr=6.9e-05\n",
      "Batch: 448/500 | Epoch  52, time=589.7ms/ 11.8ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  53, time=587.7ms/ 12.1ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  54, time=592.3ms/ 12.5ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  55, time=593.2ms/ 11.7ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  56, time=597.0ms/ 12.2ms | Training loss: 1.370 | Learning rate: 0.000 | lr=2.3e-05\n",
      "Batch: 448/500 | Epoch  57, time=588.2ms/ 12.2ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  58, time=596.3ms/ 11.8ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  59, time=590.2ms/ 12.3ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  60, time=588.7ms/ 12.1ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  61, time=594.7ms/ 11.7ms | Training loss: 1.370 | Learning rate: 0.000 | lr=7.6e-06\n",
      "Batch: 448/500 | Epoch  62, time=596.5ms/ 11.7ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  63, time=586.1ms/ 12.2ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  64, time=587.7ms/ 14.1ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  65, time=591.7ms/ 12.5ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  66, time=603.4ms/ 12.1ms | Training loss: 1.370 | Learning rate: 0.000 | lr=2.5e-06\n",
      "Batch: 448/500 | Epoch  67, time=592.3ms/ 11.8ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  68, time=591.2ms/ 12.6ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  69, time=591.5ms/ 11.5ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  70, time=577.1ms/ 12.3ms | Training loss: 1.370 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  71, time=578.0ms/ 12.4ms | Training loss: 1.370 | Learning rate: 0.000 | lr=8.5e-07\n",
      "____________________________________________________________________________________________________\n",
      "Test on task  0 - Vacuum cleaning: loss=1.436\n",
      "Test on task  1 - Mopping the floor: loss=1.541\n",
      "Test on task  2 - Carry warm food: loss=1.401\n",
      "Saving at ../checkpoints_16_tasks/16_task_groups_PUGCL\n",
      "****************************************************************************************************\n",
      "Task  3 (Carry cold food)\n",
      "****************************************************************************************************\n",
      "Starting training for the tasks in group:  3\n",
      "Batch: 448/500 | Epoch   1, time=578.3ms/ 11.7ms | Training loss: 1.511 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   2, time=578.5ms/ 11.7ms | Training loss: 1.432 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   3, time=581.2ms/ 11.9ms | Training loss: 1.434 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch   4, time=581.5ms/ 11.7ms | Training loss: 1.394 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   5, time=580.5ms/ 11.5ms | Training loss: 1.385 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   6, time=581.9ms/ 11.7ms | Training loss: 1.351 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   7, time=582.0ms/ 11.6ms | Training loss: 1.373 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch   8, time=580.7ms/ 11.7ms | Training loss: 1.345 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   9, time=582.8ms/ 12.1ms | Training loss: 1.333 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  10, time=579.7ms/ 11.6ms | Training loss: 1.327 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  11, time=593.0ms/ 11.9ms | Training loss: 1.313 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  12, time=584.3ms/ 12.4ms | Training loss: 1.335 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  13, time=584.0ms/ 11.8ms | Training loss: 1.317 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  14, time=587.6ms/ 11.8ms | Training loss: 1.308 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  15, time=253602.7ms/ 17.9ms | Training loss: 1.307 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  16, time=617.8ms/ 14.7ms | Training loss: 1.304 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  17, time=2177.7ms/134.5ms | Training loss: 1.290 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  18, time=4811.8ms/ 43.5ms | Training loss: 1.294 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  19, time=1079.3ms/ 22.9ms | Training loss: 1.288 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  20, time=791.7ms/ 19.8ms | Training loss: 1.274 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  21, time=810.2ms/ 17.7ms | Training loss: 1.349 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  22, time=271594.8ms/ 12.1ms | Training loss: 1.322 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  23, time=587.7ms/ 11.7ms | Training loss: 1.299 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  24, time=1934.2ms/117.0ms | Training loss: 1.272 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  25, time=4465.0ms/ 41.5ms | Training loss: 1.288 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  26, time=1142.9ms/ 17.9ms | Training loss: 1.260 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  27, time=710.9ms/ 15.1ms | Training loss: 1.259 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  28, time=666.4ms/ 15.5ms | Training loss: 1.265 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  29, time=804.2ms/ 17.5ms | Training loss: 1.268 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  30, time=780.8ms/ 15.7ms | Training loss: 1.257 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  31, time=745.0ms/ 15.9ms | Training loss: 1.258 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  32, time=751.3ms/ 16.1ms | Training loss: 1.251 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  33, time=770.7ms/ 16.2ms | Training loss: 1.251 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  34, time=757.7ms/ 16.1ms | Training loss: 1.274 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  35, time=767.0ms/ 16.8ms | Training loss: 1.258 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  36, time=835.9ms/ 17.7ms | Training loss: 1.273 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  37, time=868.3ms/ 19.0ms | Training loss: 1.257 | Learning rate: 0.050 | lr=1.7e-02\n",
      "Batch: 448/500 | Epoch  38, time=856.6ms/ 19.8ms | Training loss: 1.257 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  39, time=798.8ms/ 16.3ms | Training loss: 1.257 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  40, time=773.9ms/ 16.4ms | Training loss: 1.257 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  41, time=775.3ms/ 16.4ms | Training loss: 1.257 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  42, time=773.5ms/ 16.1ms | Training loss: 1.257 | Learning rate: 0.017 | lr=5.6e-03\n",
      "Batch: 448/500 | Epoch  43, time=774.3ms/ 16.5ms | Training loss: 1.257 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  44, time=781.5ms/ 16.9ms | Training loss: 1.257 | Learning rate: 0.006 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 448/500 | Epoch  45, time=820.8ms/ 17.4ms | Training loss: 1.257 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  46, time=841.3ms/ 17.7ms | Training loss: 1.257 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  47, time=835.1ms/ 16.8ms | Training loss: 1.257 | Learning rate: 0.006 | lr=1.9e-03\n",
      "Batch: 448/500 | Epoch  48, time=794.7ms/ 16.3ms | Training loss: 1.257 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  49, time=772.8ms/ 16.3ms | Training loss: 1.257 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  50, time=777.7ms/ 16.9ms | Training loss: 1.257 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  51, time=772.4ms/ 16.2ms | Training loss: 1.257 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  52, time=803.4ms/ 16.7ms | Training loss: 1.257 | Learning rate: 0.002 | lr=6.2e-04\n",
      "Batch: 448/500 | Epoch  53, time=836.8ms/ 17.7ms | Training loss: 1.257 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  54, time=842.2ms/ 18.1ms | Training loss: 1.257 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  55, time=819.1ms/ 17.0ms | Training loss: 1.257 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  56, time=778.5ms/ 16.2ms | Training loss: 1.257 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  57, time=772.4ms/ 16.1ms | Training loss: 1.257 | Learning rate: 0.001 | lr=2.1e-04\n",
      "Batch: 448/500 | Epoch  58, time=772.8ms/ 16.0ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  59, time=797.4ms/ 17.3ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  60, time=837.6ms/ 17.6ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  61, time=834.6ms/ 17.5ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  62, time=825.7ms/ 17.1ms | Training loss: 1.257 | Learning rate: 0.000 | lr=6.9e-05\n",
      "Batch: 448/500 | Epoch  63, time=787.9ms/ 16.0ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  64, time=770.0ms/ 19.5ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  65, time=772.5ms/ 16.0ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  66, time=788.6ms/ 16.9ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  67, time=826.8ms/ 17.7ms | Training loss: 1.257 | Learning rate: 0.000 | lr=2.3e-05\n",
      "Batch: 448/500 | Epoch  68, time=840.0ms/ 17.8ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  69, time=859.2ms/ 17.8ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  70, time=825.0ms/ 16.8ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  71, time=792.6ms/ 17.0ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  72, time=776.0ms/ 16.5ms | Training loss: 1.257 | Learning rate: 0.000 | lr=7.6e-06\n",
      "Batch: 448/500 | Epoch  73, time=779.2ms/ 16.0ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  74, time=810.5ms/ 17.5ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  75, time=837.2ms/ 17.4ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  76, time=848.3ms/ 18.0ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  77, time=837.4ms/ 17.0ms | Training loss: 1.257 | Learning rate: 0.000 | lr=2.5e-06\n",
      "Batch: 448/500 | Epoch  78, time=802.7ms/ 16.5ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  79, time=775.7ms/ 16.4ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  80, time=773.8ms/ 15.9ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  81, time=794.3ms/ 17.0ms | Training loss: 1.257 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  82, time=831.8ms/ 17.7ms | Training loss: 1.257 | Learning rate: 0.000 | lr=8.5e-07\n",
      "____________________________________________________________________________________________________\n",
      "Test on task  0 - Vacuum cleaning: loss=1.504\n",
      "Test on task  1 - Mopping the floor: loss=1.545\n",
      "Test on task  2 - Carry warm food: loss=1.416\n",
      "Test on task  3 - Carry cold food: loss=1.341\n",
      "Saving at ../checkpoints_16_tasks/16_task_groups_PUGCL\n",
      "****************************************************************************************************\n",
      "Task  4 (Carry drinks)\n",
      "****************************************************************************************************\n",
      "Starting training for the tasks in group:  4\n",
      "Batch: 448/500 | Epoch   1, time=845.2ms/ 18.0ms | Training loss: 4.853 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch   2, time=840.3ms/ 18.1ms | Training loss: 4.655 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch   3, time=814.0ms/ 17.5ms | Training loss: 4.587 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch   4, time=778.0ms/ 16.2ms | Training loss: 4.582 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch   5, time=775.6ms/ 16.7ms | Training loss: 4.541 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch   6, time=786.9ms/ 17.0ms | Training loss: 4.534 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch   7, time=823.9ms/ 20.1ms | Training loss: 4.529 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch   8, time=839.9ms/ 17.6ms | Training loss: 4.452 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch   9, time=840.0ms/ 17.8ms | Training loss: 4.414 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  10, time=825.3ms/ 16.8ms | Training loss: 4.354 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  11, time=787.7ms/ 18.0ms | Training loss: 4.334 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  12, time=774.0ms/ 16.2ms | Training loss: 4.237 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  13, time=776.7ms/ 17.0ms | Training loss: 4.203 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  14, time=820.8ms/ 17.9ms | Training loss: 4.095 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  15, time=841.6ms/ 17.5ms | Training loss: 4.055 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  16, time=839.8ms/ 17.7ms | Training loss: 4.033 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  17, time=838.4ms/ 17.8ms | Training loss: 3.985 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  18, time=808.3ms/ 16.7ms | Training loss: 3.920 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  19, time=776.7ms/ 16.1ms | Training loss: 3.830 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  20, time=773.2ms/ 18.0ms | Training loss: 3.801 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  21, time=789.4ms/ 17.0ms | Training loss: 3.744 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  22, time=832.5ms/ 17.9ms | Training loss: 3.635 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  23, time=838.9ms/ 20.2ms | Training loss: 3.624 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  24, time=838.0ms/ 17.9ms | Training loss: 3.548 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  25, time=821.7ms/ 17.5ms | Training loss: 3.489 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  26, time=788.2ms/ 16.4ms | Training loss: 3.447 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  27, time=774.7ms/ 16.8ms | Training loss: 3.386 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  28, time=780.1ms/ 18.6ms | Training loss: 3.286 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  29, time=820.0ms/ 17.7ms | Training loss: 3.231 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  30, time=838.4ms/ 17.7ms | Training loss: 3.178 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  31, time=837.4ms/ 18.1ms | Training loss: 3.143 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  32, time=829.3ms/ 16.9ms | Training loss: 3.090 | Learning rate: 0.030 | *\n",
      "Batch: 448/500 | Epoch  33, time=797.2ms/ 16.7ms | Training loss: 1.806 | Learning rate: 0.080 | *\n",
      "Batch: 448/500 | Epoch  34, time=777.7ms/ 16.2ms | Training loss: 1.474 | Learning rate: 0.080 | *\n",
      "Batch: 448/500 | Epoch  35, time=773.6ms/ 16.7ms | Training loss: 1.786 | Learning rate: 0.080 |\n",
      "Batch: 448/500 | Epoch  36, time=815.2ms/ 17.8ms | Training loss: 1.518 | Learning rate: 0.080 |\n",
      "Batch: 448/500 | Epoch  37, time=843.8ms/ 17.8ms | Training loss: 1.524 | Learning rate: 0.080 |\n",
      "Batch: 448/500 | Epoch  38, time=835.6ms/ 17.8ms | Training loss: 1.484 | Learning rate: 0.080 |\n",
      "Batch: 448/500 | Epoch  39, time=838.2ms/ 17.8ms | Training loss: 1.430 | Learning rate: 0.080 | *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 448/500 | Epoch  40, time=820.9ms/ 18.5ms | Training loss: 1.460 | Learning rate: 0.080 |\n",
      "Batch: 448/500 | Epoch  41, time=792.0ms/ 17.6ms | Training loss: 1.472 | Learning rate: 0.080 |\n",
      "Batch: 448/500 | Epoch  42, time=772.8ms/ 16.6ms | Training loss: 1.422 | Learning rate: 0.080 | *\n",
      "Batch: 448/500 | Epoch  43, time=786.8ms/ 16.8ms | Training loss: 1.569 | Learning rate: 0.080 |\n",
      "Batch: 448/500 | Epoch  44, time=835.9ms/ 17.6ms | Training loss: 1.427 | Learning rate: 0.080 |\n",
      "Batch: 448/500 | Epoch  45, time=838.9ms/ 18.2ms | Training loss: 1.467 | Learning rate: 0.080 |\n",
      "Batch: 448/500 | Epoch  46, time=837.7ms/ 19.9ms | Training loss: 1.423 | Learning rate: 0.080 |\n",
      "Batch: 448/500 | Epoch  47, time=832.6ms/ 16.9ms | Training loss: 1.380 | Learning rate: 0.080 | *\n",
      "Batch: 448/500 | Epoch  48, time=799.5ms/ 16.2ms | Training loss: 1.310 | Learning rate: 0.080 | *\n",
      "Batch: 448/500 | Epoch  49, time=773.7ms/ 16.1ms | Training loss: 1.455 | Learning rate: 0.080 |\n",
      "Batch: 448/500 | Epoch  50, time=771.9ms/ 16.2ms | Training loss: 1.365 | Learning rate: 0.080 |\n",
      "Batch: 448/500 | Epoch  51, time=809.3ms/ 18.3ms | Training loss: 1.381 | Learning rate: 0.080 |\n",
      "Batch: 448/500 | Epoch  52, time=838.4ms/ 17.5ms | Training loss: 1.470 | Learning rate: 0.080 |\n",
      "Batch: 448/500 | Epoch  53, time=835.2ms/ 17.6ms | Training loss: 1.421 | Learning rate: 0.080 | lr=2.7e-02\n",
      "Batch: 448/500 | Epoch  54, time=840.0ms/ 17.3ms | Training loss: 1.421 | Learning rate: 0.027 |\n",
      "Batch: 448/500 | Epoch  55, time=809.1ms/ 17.6ms | Training loss: 1.421 | Learning rate: 0.027 |\n",
      "Batch: 448/500 | Epoch  56, time=778.2ms/ 16.1ms | Training loss: 1.421 | Learning rate: 0.027 |\n",
      "Batch: 448/500 | Epoch  57, time=781.4ms/ 16.2ms | Training loss: 1.421 | Learning rate: 0.027 |\n",
      "Batch: 448/500 | Epoch  58, time=798.9ms/ 19.0ms | Training loss: 1.421 | Learning rate: 0.027 | lr=8.9e-03\n",
      "Batch: 448/500 | Epoch  59, time=836.7ms/ 20.5ms | Training loss: 1.421 | Learning rate: 0.009 |\n",
      "Batch: 448/500 | Epoch  60, time=839.6ms/ 17.9ms | Training loss: 1.421 | Learning rate: 0.009 |\n",
      "Batch: 448/500 | Epoch  61, time=836.0ms/ 17.6ms | Training loss: 1.421 | Learning rate: 0.009 |\n",
      "Batch: 448/500 | Epoch  62, time=821.6ms/ 16.8ms | Training loss: 1.421 | Learning rate: 0.009 |\n",
      "Batch: 448/500 | Epoch  63, time=791.7ms/ 16.0ms | Training loss: 1.421 | Learning rate: 0.009 | lr=3.0e-03\n",
      "Batch: 448/500 | Epoch  64, time=772.5ms/ 16.3ms | Training loss: 1.421 | Learning rate: 0.003 |\n",
      "Batch: 448/500 | Epoch  65, time=779.5ms/ 17.1ms | Training loss: 1.421 | Learning rate: 0.003 |\n",
      "Batch: 448/500 | Epoch  66, time=823.5ms/ 17.6ms | Training loss: 1.421 | Learning rate: 0.003 |\n",
      "Batch: 448/500 | Epoch  67, time=836.3ms/ 17.7ms | Training loss: 1.421 | Learning rate: 0.003 |\n",
      "Batch: 448/500 | Epoch  68, time=840.0ms/ 17.8ms | Training loss: 1.421 | Learning rate: 0.003 | lr=9.9e-04\n",
      "Batch: 448/500 | Epoch  69, time=840.1ms/ 17.5ms | Training loss: 1.421 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  70, time=802.8ms/ 18.4ms | Training loss: 1.421 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  71, time=773.1ms/ 16.4ms | Training loss: 1.421 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  72, time=774.3ms/ 16.2ms | Training loss: 1.421 | Learning rate: 0.001 |\n",
      "Batch: 448/500 | Epoch  73, time=802.3ms/ 19.1ms | Training loss: 1.421 | Learning rate: 0.001 | lr=3.3e-04\n",
      "Batch: 448/500 | Epoch  74, time=838.8ms/ 18.1ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  75, time=836.6ms/ 17.5ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  76, time=836.2ms/ 18.0ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  77, time=826.5ms/ 22.3ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  78, time=806.7ms/ 17.0ms | Training loss: 1.421 | Learning rate: 0.000 | lr=1.1e-04\n",
      "Batch: 448/500 | Epoch  79, time=803.1ms/ 16.9ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  80, time=803.7ms/ 17.7ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  81, time=807.1ms/ 16.7ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  82, time=809.1ms/ 16.8ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  83, time=805.3ms/ 16.9ms | Training loss: 1.421 | Learning rate: 0.000 | lr=3.7e-05\n",
      "Batch: 448/500 | Epoch  84, time=803.4ms/ 17.2ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  85, time=858.2ms/ 20.7ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  86, time=842.7ms/ 17.7ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  87, time=843.5ms/ 17.6ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  88, time=824.8ms/ 17.2ms | Training loss: 1.421 | Learning rate: 0.000 | lr=1.2e-05\n",
      "Batch: 448/500 | Epoch  89, time=800.3ms/ 16.3ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  90, time=774.3ms/ 16.3ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  91, time=778.9ms/ 17.0ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  92, time=824.2ms/ 17.7ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  93, time=838.7ms/ 17.5ms | Training loss: 1.421 | Learning rate: 0.000 | lr=4.1e-06\n",
      "Batch: 448/500 | Epoch  94, time=837.2ms/ 18.3ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  95, time=837.0ms/ 16.9ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  96, time=806.8ms/ 16.9ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  97, time=778.4ms/ 16.4ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch  98, time=773.8ms/ 16.5ms | Training loss: 1.421 | Learning rate: 0.000 | lr=1.4e-06\n",
      "Batch: 448/500 | Epoch  99, time=800.1ms/ 21.3ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 100, time=840.8ms/ 17.5ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 101, time=838.1ms/ 17.9ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 102, time=838.8ms/ 18.0ms | Training loss: 1.421 | Learning rate: 0.000 |\n",
      "Batch: 448/500 | Epoch 103, time=703.2ms/ 13.2ms | Training loss: 1.421 | Learning rate: 0.000 | lr=4.5e-07\n",
      "____________________________________________________________________________________________________\n",
      "Test on task  0 - Vacuum cleaning: loss=1.582\n",
      "Test on task  1 - Mopping the floor: loss=2.210\n",
      "Test on task  2 - Carry warm food: loss=1.415\n",
      "Test on task  3 - Carry cold food: loss=1.569\n",
      "Test on task  4 - Carry drinks   : loss=1.314\n",
      "Saving at ../checkpoints_16_tasks/16_task_groups_PUGCL\n",
      "****************************************************************************************************\n",
      "Task  5 (Carry small objects (plates, toys))\n",
      "****************************************************************************************************\n",
      "Starting training for the tasks in group:  5\n",
      "Batch: 448/500 | Epoch   1, time=616.8ms/ 12.1ms | Training loss: 1.718 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   2, time=597.9ms/ 13.3ms | Training loss: 1.561 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   3, time=602.3ms/ 12.3ms | Training loss: 1.525 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   4, time=590.4ms/ 12.1ms | Training loss: 1.474 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   5, time=632.6ms/ 12.3ms | Training loss: 1.463 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   6, time=593.0ms/ 12.2ms | Training loss: 1.467 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch   7, time=590.0ms/ 12.3ms | Training loss: 1.438 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   8, time=589.1ms/ 11.9ms | Training loss: 1.409 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch   9, time=592.8ms/ 12.1ms | Training loss: 1.402 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  10, time=589.4ms/ 12.1ms | Training loss: 1.392 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  11, time=588.5ms/ 11.8ms | Training loss: 1.405 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  12, time=591.8ms/ 12.3ms | Training loss: 1.454 | Learning rate: 0.050 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 448/500 | Epoch  13, time=590.0ms/ 12.1ms | Training loss: 1.400 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  14, time=597.2ms/ 12.0ms | Training loss: 1.408 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  15, time=588.4ms/ 12.9ms | Training loss: 1.382 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  16, time=585.5ms/ 11.8ms | Training loss: 1.395 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  17, time=585.2ms/ 12.2ms | Training loss: 1.361 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  18, time=588.6ms/ 11.8ms | Training loss: 1.360 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  19, time=584.7ms/ 13.3ms | Training loss: 1.370 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  20, time=585.1ms/ 12.2ms | Training loss: 1.363 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  21, time=587.5ms/ 11.8ms | Training loss: 1.348 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  22, time=585.1ms/ 12.6ms | Training loss: 1.343 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  23, time=586.6ms/ 12.1ms | Training loss: 1.337 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  24, time=585.3ms/ 11.9ms | Training loss: 1.347 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  25, time=590.8ms/ 11.8ms | Training loss: 1.353 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  26, time=588.6ms/ 11.7ms | Training loss: 1.335 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  27, time=595.8ms/ 12.7ms | Training loss: 1.331 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  28, time=586.7ms/ 12.3ms | Training loss: 1.359 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  29, time=583.1ms/ 11.7ms | Training loss: 1.325 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  30, time=622.5ms/ 12.2ms | Training loss: 1.335 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  31, time=593.0ms/ 12.3ms | Training loss: 1.341 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  32, time=596.5ms/ 11.6ms | Training loss: 1.319 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  33, time=591.7ms/ 12.4ms | Training loss: 1.333 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  34, time=594.7ms/ 12.6ms | Training loss: 1.353 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  35, time=597.5ms/ 11.9ms | Training loss: 1.314 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  36, time=595.5ms/ 12.3ms | Training loss: 1.318 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  37, time=595.9ms/ 12.2ms | Training loss: 1.320 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  38, time=601.3ms/ 13.1ms | Training loss: 1.309 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  39, time=602.1ms/ 12.9ms | Training loss: 1.317 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  40, time=584.0ms/ 12.0ms | Training loss: 1.353 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  41, time=583.6ms/ 11.6ms | Training loss: 1.309 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  42, time=583.4ms/ 11.8ms | Training loss: 1.309 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  43, time=584.1ms/ 11.5ms | Training loss: 1.316 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  44, time=582.4ms/ 12.0ms | Training loss: 1.306 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  45, time=583.3ms/ 12.0ms | Training loss: 1.307 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  46, time=582.1ms/ 17.3ms | Training loss: 1.322 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  47, time=591.1ms/ 12.2ms | Training loss: 1.325 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  48, time=593.9ms/ 12.1ms | Training loss: 1.305 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  49, time=594.8ms/ 12.2ms | Training loss: 1.299 | Learning rate: 0.050 | *\n",
      "Batch: 448/500 | Epoch  50, time=595.1ms/ 13.2ms | Training loss: 1.306 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  51, time=595.6ms/ 12.2ms | Training loss: 1.325 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  52, time=594.8ms/ 12.2ms | Training loss: 1.305 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  53, time=596.9ms/ 11.7ms | Training loss: 1.323 | Learning rate: 0.050 |\n",
      "Batch: 448/500 | Epoch  54, time=584.6ms/ 11.5ms | Training loss: 1.308 | Learning rate: 0.050 | lr=1.7e-02\n",
      "Batch: 448/500 | Epoch  55, time=584.8ms/ 11.7ms | Training loss: 1.308 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  56, time=585.3ms/ 11.6ms | Training loss: 1.308 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  57, time=585.2ms/ 12.0ms | Training loss: 1.308 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  58, time=582.8ms/ 11.8ms | Training loss: 1.308 | Learning rate: 0.017 |\n",
      "Batch: 448/500 | Epoch  59, time=586.1ms/ 11.6ms | Training loss: 1.308 | Learning rate: 0.017 | lr=5.6e-03\n",
      "Batch: 448/500 | Epoch  60, time=582.8ms/ 11.8ms | Training loss: 1.308 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  61, time=611.5ms/ 13.1ms | Training loss: 1.308 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  62, time=653.3ms/ 11.8ms | Training loss: 1.308 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  63, time=606.9ms/ 12.3ms | Training loss: 1.308 | Learning rate: 0.006 |\n",
      "Batch: 448/500 | Epoch  64, time=602.0ms/ 12.2ms | Training loss: 1.308 | Learning rate: 0.006 | lr=1.9e-03\n",
      "Batch: 448/500 | Epoch  65, time=591.3ms/ 11.9ms | Training loss: 1.308 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  66, time=615.9ms/ 13.3ms | Training loss: 1.308 | Learning rate: 0.002 |\n",
      "Batch: 448/500 | Epoch  67, time=613.9ms/ 13.7ms | Training loss: 1.308 | Learning rate: 0.002 |\n",
      "Batch: 320/500 \n",
      "____________________________________________________________________________________________________\n",
      "Test on task  0 - Vacuum cleaning: loss=1.575\n",
      "Test on task  1 - Mopping the floor: loss=2.128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the two tasks:\n",
    "loss = np.zeros((len(task_outputs), len(task_outputs)), dtype=np.float32)\n",
    "for task, n_class in task_outputs[args.sti:]:\n",
    "    print('*'*100)\n",
    "    print('Task {:2d} ({:s})'.format(task, data[task]['name']))\n",
    "    print('*'*100)\n",
    "\n",
    "    # Get data:\n",
    "    xtrain = data[task]['train']['x'][:,1:].type(torch.float32).to(args.device)\n",
    "    ytrain = data[task]['train']['y'].type(torch.float32).to(args.device)\n",
    "    xvalid = data[task]['valid']['x'][:,1:].type(torch.float32).to(args.device)\n",
    "    yvalid = data[task]['valid']['y'].type(torch.float32).to(args.device)\n",
    "\n",
    "    # Start training\n",
    "    print(\"Starting training for the tasks in group: \", task)\n",
    "    approach.train(task, xtrain, ytrain, xvalid, yvalid)\n",
    "    print('_'*100)\n",
    "\n",
    "    # Validate for this task group:\n",
    "    for u in range(task+1):\n",
    "        xtest = data[u]['test']['x'][:,1:].type(torch.float32).to(args.device)\n",
    "        ytest = data[u]['test']['y'].type(torch.float32).to(args.device)\n",
    "        test_loss = approach.eval(u, xtest, ytest, debug=True)\n",
    "        print(\"Test on task {:2d} - {:15s}: loss={:.3f}\".format(u, data[u]['name'], test_loss))\n",
    "        loss[task, u] = test_loss\n",
    "\n",
    "    # Save\n",
    "    print(\"Saving at \" + args.checkpoint)\n",
    "    np.savetxt(os.path.join(args.checkpoint, '{}_{}_{}.txt'.format(args.experiment, args.approach, args.seed)), loss, '%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
